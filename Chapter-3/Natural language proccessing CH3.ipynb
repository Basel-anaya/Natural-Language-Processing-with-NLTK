{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1d84bff",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab3ed613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a431148f",
   "metadata": {},
   "source": [
    "## Accessing Text from the Web and from Disk\n",
    "\n",
    "A small sample of texts from Project Gutenberg appears in the NLTK corpus collection. However, you may be interested in analyzing other texts from Project Gutenberg. You can browse the catalog of 25,000 free online books at http://www.gutenberg.org/catalog/, and obtain a URL to an ASCII text file.\n",
    "\n",
    "Text number 2554 is an English translation of Crime and Punishment, and we can access it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ad63d56",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "import requests\n",
    "# Import urlopen from urllib2 module\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "# Specify url as this particular text string\n",
    "response = urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "type(raw)\n",
    "# what is the type of the raw string data we read? Unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66012d19",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1176812"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw)\n",
    "# number of characters (including spaces) from this text file from the web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f9a44b8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg eBook of Crime and Punishment, by Fyodor Dostoevsky\\r'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw[:75]\n",
    "# what are the first 75 characters from this text file?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7407deb",
   "metadata": {},
   "source": [
    "For our language processing, we want to break up the string into words and punctuation, as we saw in 1.. This step is called tokenization, and it produces our familiar structure, a list of words and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "53d31cac",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(raw)\n",
    "# word_tokenize converts raw string data into word tokens\n",
    "type(tokens)\n",
    "# Shows that these tokens are placed in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b576fa9c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257058"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)\n",
    "# Return the number of word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "28adfd8c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeffThe',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'eBook',\n",
       " 'of',\n",
       " 'Crime',\n",
       " 'and',\n",
       " 'Punishment',\n",
       " ',',\n",
       " 'by']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:10]\n",
    "# let's return the first 10 words/tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04b1436",
   "metadata": {},
   "source": [
    "Notice that NLTK was needed for tokenization, but not for any of the earlier tasks of opening a URL and reading it into a string. If we now take the further step of creating an NLTK text from this list, we can carry out all of the other linguistic processing we saw in 1., along with the regular list operations like slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d19a12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.Text(tokens)\n",
    "# Convert tokens list into a format NLTK can understand and process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "efccb2f2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.text.Text"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)\n",
    "# Now we have an NLTK text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aacd9760",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['insight',\n",
       " 'impresses',\n",
       " 'us',\n",
       " 'as',\n",
       " 'wisdom',\n",
       " '...',\n",
       " 'that',\n",
       " 'wisdom',\n",
       " 'of',\n",
       " 'the',\n",
       " 'heart',\n",
       " 'which',\n",
       " 'we',\n",
       " 'seek',\n",
       " 'that',\n",
       " 'we',\n",
       " 'may',\n",
       " 'learn',\n",
       " 'from',\n",
       " 'it',\n",
       " 'how',\n",
       " 'to',\n",
       " 'live',\n",
       " '.',\n",
       " 'All',\n",
       " 'his',\n",
       " 'other',\n",
       " 'gifts',\n",
       " 'came',\n",
       " 'to',\n",
       " 'him',\n",
       " 'from',\n",
       " 'nature',\n",
       " ',',\n",
       " 'this',\n",
       " 'he',\n",
       " 'won',\n",
       " 'for']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[1024:1062]\n",
    "# return this subset of words/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "755d6280",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Katerina Ivanovna; Pyotr Petrovitch; Pulcheria Alexandrovna; Avdotya\n",
      "Romanovna; Rodion Romanovitch; Marfa Petrovna; Sofya Semyonovna; old\n",
      "woman; Project Gutenberg-tm; Porfiry Petrovitch; Amalia Ivanovna;\n",
      "great deal; young man; Nikodim Fomitch; Project Gutenberg; Ilya\n",
      "Petrovitch; Andrey Semyonovitch; Hay Market; Dmitri Prokofitch; Good\n",
      "heavens\n"
     ]
    }
   ],
   "source": [
    "text.collocations()\n",
    "# Remember, \"Collocations are expressions of multiple words which commonly co-occur.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280b5964",
   "metadata": {},
   "source": [
    "Notice that Project Gutenberg appears as a collocation. This is because each text downloaded from Project Gutenberg contains a \n",
    "header with the name of the text, the author, the names of people who scanned and corrected the text, a license, and so on. \n",
    "Sometimes this information appears in a footer at the end of the file. We cannot reliably detect where the content begins and ends, \n",
    "and so have to resort to manual inspection of the file, to discover unique strings that mark the beginning and the end, before \n",
    "trimming raw to be just the content and nothing else:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7081a377",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5575"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.find(\"PART I\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "66f42272",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.rfind(\"End of Project Gutenberg's Crime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "482c9dfa",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we essentially subset raw to be the \"raw\" content, and no header/metadata.\n",
    "\n",
    "raw = raw[5338:1157743]\n",
    "raw.find(\"PART I\")\n",
    "\n",
    "# The find() and rfind() (\"reverse find\") methods help us get the right index values to use for slicing the string [1]. We overwrite \n",
    "# raw with this slice, so now it begins with \"PART I\" and goes up to (but not including) the phrase that marks the end of the content.\n",
    "\n",
    "# This was our first brush with the reality of the web: texts found on the web may contain unwanted material, and there may not be an \n",
    "# automatic way to remove it. But with a small amount of extra work we can extract the material we need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbee33bb",
   "metadata": {},
   "source": [
    "## Dealing with HTML\n",
    "\n",
    "Much of the text on the web is in the form of HTML documents. You can use a web browser to save a page as text to a local file, \n",
    "then access this as described in the section on files below. However, if you're going to do this often, it's easiest to get Python \n",
    "to do the work directly. The first step is the same as before, using urlopen. For fun we'll pick a BBC News story called Blondes to \n",
    "die out in 200 years, an urban legend passed along by the BBC as established scientific fact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "09bdf328",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
    "\n",
    "# Remember, we use urlopen instead of request.urlopen\n",
    "html = urlopen(url).read().decode('utf8')\n",
    "html[:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7156f820",
   "metadata": {},
   "source": [
    "You can type print(html) to see the HTML content in all its glory, including meta tags, an image map, JavaScript, forms, and tables.\n",
    "To get text out of HTML we will use a Python library called BeautifulSoup, available from \n",
    "http://www.crummy.com/software/BeautifulSoup/:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d642cfb2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BBC',\n",
       " 'NEWS',\n",
       " '|',\n",
       " 'Health',\n",
       " '|',\n",
       " 'Blondes',\n",
       " \"'to\",\n",
       " 'die',\n",
       " 'out',\n",
       " 'in',\n",
       " '200',\n",
       " \"years'\",\n",
       " 'NEWS',\n",
       " 'SPORT',\n",
       " 'WEATHER',\n",
       " 'WORLD',\n",
       " 'SERVICE',\n",
       " 'A-Z',\n",
       " 'INDEX',\n",
       " 'SEARCH',\n",
       " 'You',\n",
       " 'are',\n",
       " 'in',\n",
       " ':',\n",
       " 'Health',\n",
       " 'News',\n",
       " 'Front',\n",
       " 'Page',\n",
       " 'Africa',\n",
       " 'Americas',\n",
       " 'Asia-Pacific',\n",
       " 'Europe',\n",
       " 'Middle',\n",
       " 'East',\n",
       " 'South',\n",
       " 'Asia',\n",
       " 'UK',\n",
       " 'Business',\n",
       " 'Entertainment',\n",
       " 'Science/Nature',\n",
       " 'Technology',\n",
       " 'Health',\n",
       " 'Medical',\n",
       " 'notes',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '-',\n",
       " 'Talking',\n",
       " 'Point',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '-',\n",
       " 'Country',\n",
       " 'Profiles',\n",
       " 'In',\n",
       " 'Depth',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '-',\n",
       " 'Programmes',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '-',\n",
       " 'SERVICES',\n",
       " 'Daily',\n",
       " 'E-mail',\n",
       " 'News',\n",
       " 'Ticker',\n",
       " 'Mobile/PDAs',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '-',\n",
       " 'Text',\n",
       " 'Only',\n",
       " 'Feedback',\n",
       " 'Help',\n",
       " 'EDITIONS',\n",
       " 'Change',\n",
       " 'to',\n",
       " 'UK',\n",
       " 'Friday',\n",
       " ',',\n",
       " '27',\n",
       " 'September',\n",
       " ',',\n",
       " '2002',\n",
       " ',',\n",
       " '11:51',\n",
       " 'GMT',\n",
       " '12:51',\n",
       " 'UK',\n",
       " 'Blondes',\n",
       " \"'to\",\n",
       " 'die',\n",
       " 'out',\n",
       " 'in',\n",
       " '200',\n",
       " \"years'\",\n",
       " 'Scientists',\n",
       " 'believe',\n",
       " 'the',\n",
       " 'last',\n",
       " 'blondes',\n",
       " 'will',\n",
       " 'be',\n",
       " 'in',\n",
       " 'Finland',\n",
       " 'The',\n",
       " 'last',\n",
       " 'natural',\n",
       " 'blondes',\n",
       " 'will',\n",
       " 'die',\n",
       " 'out',\n",
       " 'within',\n",
       " '200',\n",
       " 'years',\n",
       " ',',\n",
       " 'scientists',\n",
       " 'believe',\n",
       " '.',\n",
       " 'A',\n",
       " 'study',\n",
       " 'by',\n",
       " 'experts',\n",
       " 'in',\n",
       " 'Germany',\n",
       " 'suggests',\n",
       " 'people',\n",
       " 'with',\n",
       " 'blonde',\n",
       " 'hair',\n",
       " 'are',\n",
       " 'an',\n",
       " 'endangered',\n",
       " 'species',\n",
       " 'and',\n",
       " 'will',\n",
       " 'become',\n",
       " 'extinct',\n",
       " 'by',\n",
       " '2202',\n",
       " '.',\n",
       " 'Researchers',\n",
       " 'predict',\n",
       " 'the',\n",
       " 'last',\n",
       " 'truly',\n",
       " 'natural',\n",
       " 'blonde',\n",
       " 'will',\n",
       " 'be',\n",
       " 'born',\n",
       " 'in',\n",
       " 'Finland',\n",
       " '-',\n",
       " 'the',\n",
       " 'country',\n",
       " 'with',\n",
       " 'the',\n",
       " 'highest',\n",
       " 'proportion',\n",
       " 'of',\n",
       " 'blondes',\n",
       " '.',\n",
       " 'The',\n",
       " 'frequency',\n",
       " 'of',\n",
       " 'blondes',\n",
       " 'may',\n",
       " 'drop',\n",
       " 'but',\n",
       " 'they',\n",
       " 'wo',\n",
       " \"n't\",\n",
       " 'disappear',\n",
       " 'Prof',\n",
       " 'Jonathan',\n",
       " 'Rees',\n",
       " ',',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Edinburgh',\n",
       " 'But',\n",
       " 'they',\n",
       " 'say',\n",
       " 'too',\n",
       " 'few',\n",
       " 'people',\n",
       " 'now',\n",
       " 'carry',\n",
       " 'the',\n",
       " 'gene',\n",
       " 'for',\n",
       " 'blondes',\n",
       " 'to',\n",
       " 'last',\n",
       " 'beyond',\n",
       " 'the',\n",
       " 'next',\n",
       " 'two',\n",
       " 'centuries',\n",
       " '.',\n",
       " 'The',\n",
       " 'problem',\n",
       " 'is',\n",
       " 'that',\n",
       " 'blonde',\n",
       " 'hair',\n",
       " 'is',\n",
       " 'caused',\n",
       " 'by',\n",
       " 'a',\n",
       " 'recessive',\n",
       " 'gene',\n",
       " '.',\n",
       " 'In',\n",
       " 'order',\n",
       " 'for',\n",
       " 'a',\n",
       " 'child',\n",
       " 'to',\n",
       " 'have',\n",
       " 'blonde',\n",
       " 'hair',\n",
       " ',',\n",
       " 'it',\n",
       " 'must',\n",
       " 'have',\n",
       " 'the',\n",
       " 'gene',\n",
       " 'on',\n",
       " 'both',\n",
       " 'sides',\n",
       " 'of',\n",
       " 'the',\n",
       " 'family',\n",
       " 'in',\n",
       " 'the',\n",
       " 'grandparents',\n",
       " \"'\",\n",
       " 'generation',\n",
       " '.',\n",
       " 'Dyed',\n",
       " 'rivals',\n",
       " 'The',\n",
       " 'researchers',\n",
       " 'also',\n",
       " 'believe',\n",
       " 'that',\n",
       " 'so-called',\n",
       " 'bottle',\n",
       " 'blondes',\n",
       " 'may',\n",
       " 'be',\n",
       " 'to',\n",
       " 'blame',\n",
       " 'for',\n",
       " 'the',\n",
       " 'demise',\n",
       " 'of',\n",
       " 'their',\n",
       " 'natural',\n",
       " 'rivals',\n",
       " '.',\n",
       " 'They',\n",
       " 'suggest',\n",
       " 'that',\n",
       " 'dyed-blondes',\n",
       " 'are',\n",
       " 'more',\n",
       " 'attractive',\n",
       " 'to',\n",
       " 'men',\n",
       " 'who',\n",
       " 'choose',\n",
       " 'them',\n",
       " 'as',\n",
       " 'partners',\n",
       " 'over',\n",
       " 'true',\n",
       " 'blondes',\n",
       " '.',\n",
       " 'Bottle-blondes',\n",
       " 'like',\n",
       " 'Ann',\n",
       " 'Widdecombe',\n",
       " 'may',\n",
       " 'be',\n",
       " 'to',\n",
       " 'blame',\n",
       " 'But',\n",
       " 'Jonathan',\n",
       " 'Rees',\n",
       " ',',\n",
       " 'professor',\n",
       " 'of',\n",
       " 'dermatology',\n",
       " 'at',\n",
       " 'the',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Edinburgh',\n",
       " 'said',\n",
       " 'it',\n",
       " 'was',\n",
       " 'unlikely',\n",
       " 'blondes',\n",
       " 'would',\n",
       " 'die',\n",
       " 'out',\n",
       " 'completely',\n",
       " '.',\n",
       " '``',\n",
       " 'Genes',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'die',\n",
       " 'out',\n",
       " 'unless',\n",
       " 'there',\n",
       " 'is',\n",
       " 'a',\n",
       " 'disadvantage',\n",
       " 'of',\n",
       " 'having',\n",
       " 'that',\n",
       " 'gene',\n",
       " 'or',\n",
       " 'by',\n",
       " 'chance',\n",
       " '.',\n",
       " 'They',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'disappear',\n",
       " ',',\n",
       " \"''\",\n",
       " 'he',\n",
       " 'told',\n",
       " 'BBC',\n",
       " 'News',\n",
       " 'Online',\n",
       " '.',\n",
       " '``',\n",
       " 'The',\n",
       " 'only',\n",
       " 'reason',\n",
       " 'blondes',\n",
       " 'would',\n",
       " 'disappear',\n",
       " 'is',\n",
       " 'if',\n",
       " 'having',\n",
       " 'the',\n",
       " 'gene',\n",
       " 'was',\n",
       " 'a',\n",
       " 'disadvantage',\n",
       " 'and',\n",
       " 'I',\n",
       " 'do',\n",
       " 'not',\n",
       " 'think',\n",
       " 'that',\n",
       " 'is',\n",
       " 'the',\n",
       " 'case',\n",
       " '.',\n",
       " '``',\n",
       " 'The',\n",
       " 'frequency',\n",
       " 'of',\n",
       " 'blondes',\n",
       " 'may',\n",
       " 'drop',\n",
       " 'but',\n",
       " 'they',\n",
       " 'wo',\n",
       " \"n't\",\n",
       " 'disappear',\n",
       " '.',\n",
       " \"''\",\n",
       " 'See',\n",
       " 'also',\n",
       " ':',\n",
       " '28',\n",
       " 'Mar',\n",
       " '01',\n",
       " '|',\n",
       " 'Education',\n",
       " 'What',\n",
       " 'is',\n",
       " 'it',\n",
       " 'about',\n",
       " 'blondes',\n",
       " '?',\n",
       " '09',\n",
       " 'Apr',\n",
       " '99',\n",
       " '|',\n",
       " 'Health',\n",
       " 'Platinum',\n",
       " 'blondes',\n",
       " 'are',\n",
       " 'labelled',\n",
       " 'as',\n",
       " 'dumb',\n",
       " '17',\n",
       " 'Apr',\n",
       " '02',\n",
       " '|',\n",
       " 'Health',\n",
       " 'Hair',\n",
       " 'dye',\n",
       " 'cancer',\n",
       " 'alert',\n",
       " 'Internet',\n",
       " 'links',\n",
       " ':',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Edinburgh',\n",
       " 'The',\n",
       " 'BBC',\n",
       " 'is',\n",
       " 'not',\n",
       " 'responsible',\n",
       " 'for',\n",
       " 'the',\n",
       " 'content',\n",
       " 'of',\n",
       " 'external',\n",
       " 'internet',\n",
       " 'sites',\n",
       " 'Top',\n",
       " 'Health',\n",
       " 'stories',\n",
       " 'now',\n",
       " ':',\n",
       " 'Heart',\n",
       " 'risk',\n",
       " 'link',\n",
       " 'to',\n",
       " 'big',\n",
       " 'families',\n",
       " 'Back',\n",
       " 'pain',\n",
       " 'drug',\n",
       " \"'may\",\n",
       " 'aid',\n",
       " \"diabetics'\",\n",
       " 'Congo',\n",
       " 'Ebola',\n",
       " 'outbreak',\n",
       " 'confirmed',\n",
       " 'Vegetables',\n",
       " 'ward',\n",
       " 'off',\n",
       " \"Alzheimer's\",\n",
       " 'Polio',\n",
       " 'campaign',\n",
       " 'launched',\n",
       " 'in',\n",
       " 'Iraq',\n",
       " 'Gene',\n",
       " 'defect',\n",
       " 'explains',\n",
       " 'high',\n",
       " 'blood',\n",
       " 'pressure',\n",
       " 'Botox',\n",
       " \"'may\",\n",
       " 'cause',\n",
       " 'new',\n",
       " \"wrinkles'\",\n",
       " 'Alien',\n",
       " \"'abductees\",\n",
       " \"'\",\n",
       " 'show',\n",
       " 'real',\n",
       " 'symptoms',\n",
       " 'Links',\n",
       " 'to',\n",
       " 'more',\n",
       " 'Health',\n",
       " 'stories',\n",
       " 'are',\n",
       " 'at',\n",
       " 'the',\n",
       " 'foot',\n",
       " 'of',\n",
       " 'the',\n",
       " 'page',\n",
       " '.',\n",
       " 'E-mail',\n",
       " 'this',\n",
       " 'story',\n",
       " 'to',\n",
       " 'a',\n",
       " 'friend',\n",
       " 'Links',\n",
       " 'to',\n",
       " 'more',\n",
       " 'Health',\n",
       " 'stories',\n",
       " 'In',\n",
       " 'This',\n",
       " 'Section',\n",
       " 'Heart',\n",
       " 'risk',\n",
       " 'link',\n",
       " 'to',\n",
       " 'big',\n",
       " 'families',\n",
       " 'Back',\n",
       " 'pain',\n",
       " 'drug',\n",
       " \"'may\",\n",
       " 'aid',\n",
       " \"diabetics'\",\n",
       " 'Congo',\n",
       " 'Ebola',\n",
       " 'outbreak',\n",
       " 'confirmed',\n",
       " 'Vegetables',\n",
       " 'ward',\n",
       " 'off',\n",
       " \"Alzheimer's\",\n",
       " 'Polio',\n",
       " 'campaign',\n",
       " 'launched',\n",
       " 'in',\n",
       " 'Iraq',\n",
       " 'Gene',\n",
       " 'defect',\n",
       " 'explains',\n",
       " 'high',\n",
       " 'blood',\n",
       " 'pressure',\n",
       " 'Botox',\n",
       " \"'may\",\n",
       " 'cause',\n",
       " 'new',\n",
       " \"wrinkles'\",\n",
       " 'Alien',\n",
       " \"'abductees\",\n",
       " \"'\",\n",
       " 'show',\n",
       " 'real',\n",
       " 'symptoms',\n",
       " 'How',\n",
       " 'sperm',\n",
       " 'wriggle',\n",
       " 'Bollywood',\n",
       " 'told',\n",
       " 'to',\n",
       " 'stub',\n",
       " 'it',\n",
       " 'out',\n",
       " 'Fears',\n",
       " 'over',\n",
       " 'tuna',\n",
       " 'health',\n",
       " 'risk',\n",
       " 'to',\n",
       " 'babies',\n",
       " 'Public',\n",
       " 'can',\n",
       " 'be',\n",
       " 'taught',\n",
       " 'to',\n",
       " 'spot',\n",
       " 'strokes',\n",
       " '^^',\n",
       " 'Back',\n",
       " 'to',\n",
       " 'top',\n",
       " 'News',\n",
       " 'Front',\n",
       " 'Page',\n",
       " '|',\n",
       " 'Africa',\n",
       " '|',\n",
       " 'Americas',\n",
       " '|',\n",
       " 'Asia-Pacific',\n",
       " '|',\n",
       " 'Europe',\n",
       " '|',\n",
       " 'Middle',\n",
       " 'East',\n",
       " '|',\n",
       " 'South',\n",
       " 'Asia',\n",
       " '|',\n",
       " 'UK',\n",
       " '|',\n",
       " 'Business',\n",
       " '|',\n",
       " 'Entertainment',\n",
       " '|',\n",
       " 'Science/Nature',\n",
       " '|',\n",
       " 'Technology',\n",
       " '|',\n",
       " 'Health',\n",
       " '|',\n",
       " 'Talking',\n",
       " 'Point',\n",
       " '|',\n",
       " 'Country',\n",
       " 'Profiles',\n",
       " '|',\n",
       " 'In',\n",
       " 'Depth',\n",
       " '|',\n",
       " 'Programmes',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " 'To',\n",
       " 'BBC',\n",
       " 'Sport',\n",
       " '>',\n",
       " '>',\n",
       " '|',\n",
       " 'To',\n",
       " 'BBC',\n",
       " 'Weather',\n",
       " '>',\n",
       " '>',\n",
       " '|',\n",
       " 'To',\n",
       " 'BBC',\n",
       " 'World',\n",
       " 'Service',\n",
       " '>',\n",
       " '>',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '©',\n",
       " 'MMIII',\n",
       " '|',\n",
       " 'News',\n",
       " 'Sources',\n",
       " '|',\n",
       " 'Privacy']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "raw = BeautifulSoup(html, \"lxml\").get_text() # Note: added ,\"lxml\"\n",
    "tokens = word_tokenize(raw)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "12eefc02",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 5 matches:\n",
      "hey say too few people now carry the gene for blondes to last beyond the next \n",
      "blonde hair is caused by a recessive gene . In order for a child to have blond\n",
      " have blonde hair , it must have the gene on both sides of the family in the g\n",
      "ere is a disadvantage of having that gene or by chance . They do n't disappear\n",
      "des would disappear is if having the gene was a disadvantage and I do not thin\n"
     ]
    }
   ],
   "source": [
    "# This still contains unwanted material concerning site navigation and related stories. With some trial and error you can find the \n",
    "# start and end indexes of the content and select the tokens of interest, and initialize a text as before.\n",
    "\n",
    "tokens = tokens[110:390]\n",
    "text = nltk.Text(tokens)\n",
    "text.concordance('gene')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e0e9a8",
   "metadata": {},
   "source": [
    "## Processing Search Engine Results\n",
    "\n",
    "The web can be thought of as a huge corpus of unannotated text. Web search engines provide an efficient means of searching this large \n",
    "quantity of text for relevant linguistic examples. The main advantage of search engines is size: since you are searching such a large \n",
    "set of documents, you are more likely to find any linguistic pattern you are interested in. Furthermore, you can make use of very \n",
    "specific patterns, which would only match one or two examples on a smaller example, but which might match tens of thousands of \n",
    "examples when run on the web. A second advantage of web search engines is that they are very easy to use. Thus, they provide a very \n",
    "convenient tool for quickly checking a theory, to see if it is reasonable.\n",
    "\n",
    "Unfortunately, search engines have some significant shortcomings. First, the allowable range of search patterns is severely \n",
    "restricted. Unlike local corpora, where you write programs to search for arbitrarily complex patterns, search engines generally only \n",
    "allow you to search for individual words or strings of words, sometimes with wildcards. Second, search engines give inconsistent \n",
    "results, and can give widely different figures when used at different times or in different geographical regions. When content has \n",
    "been duplicated across multiple sites, search results may be boosted. Finally, the markup in the result returned by a search engine \n",
    "may change unpredictably, breaking any pattern-based method of locating particular content (a problem which is ameliorated by the use \n",
    "of search engine APIs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d5b0177f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Language Log'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Processing RSS Feeds\n",
    "\n",
    "# The blogosphere is an important source of text, in both formal and informal registers. With the help of a Python library called the \n",
    "# Universal Feed Parser, available from https://pypi.python.org/pypi/feedparser, we can access the content of a blog, as shown below:\n",
    "\n",
    "import feedparser\n",
    "# I typed the following at the Windows Command prompt: \"pip install feedparser\"\n",
    "\n",
    "llog = feedparser.parse(\"http://languagelog.ldc.upenn.edu/nll/?feed=atom\")\n",
    "llog['feed']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1553298c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(llog.entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f617b937",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'White tongue'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post = llog.entries[2]\n",
    "post.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c144df32",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>Two days ago, I met a person who had a thick white coating on their'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = post.content[0].value\n",
    "content[:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f7d875f6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Two',\n",
       " 'days',\n",
       " 'ago',\n",
       " ',',\n",
       " 'I',\n",
       " 'met',\n",
       " 'a',\n",
       " 'person',\n",
       " 'who',\n",
       " 'had',\n",
       " 'a',\n",
       " 'thick',\n",
       " 'white',\n",
       " 'coating',\n",
       " 'on',\n",
       " 'their',\n",
       " 'tongue',\n",
       " '.',\n",
       " 'Wondering',\n",
       " 'what',\n",
       " 'it',\n",
       " 'was',\n",
       " 'called',\n",
       " 'and',\n",
       " 'its',\n",
       " 'implications',\n",
       " 'for',\n",
       " 'health',\n",
       " ',',\n",
       " 'I',\n",
       " 'asked',\n",
       " 'members',\n",
       " 'of',\n",
       " 'the',\n",
       " 'e-Mair',\n",
       " 'list',\n",
       " 'about',\n",
       " 'it',\n",
       " '.',\n",
       " 'Here',\n",
       " 'are',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'answers',\n",
       " 'I',\n",
       " 'received',\n",
       " ':',\n",
       " 'Denis',\n",
       " '(',\n",
       " 'Sinologist',\n",
       " ')',\n",
       " ':',\n",
       " 'Thick',\n",
       " 'tongue',\n",
       " 'coating',\n",
       " ',',\n",
       " 'often',\n",
       " 'due',\n",
       " 'to',\n",
       " 'lengthening',\n",
       " 'of',\n",
       " 'the',\n",
       " 'keratinous',\n",
       " 'papillae',\n",
       " 'on',\n",
       " 'the',\n",
       " 'tongue',\n",
       " \"'s\",\n",
       " 'surface',\n",
       " '.',\n",
       " 'Heidi',\n",
       " '(',\n",
       " 'Yoga',\n",
       " 'teacher',\n",
       " 'and',\n",
       " 'Ayurveda',\n",
       " 'specialist',\n",
       " ')',\n",
       " ':',\n",
       " 'We',\n",
       " 'call',\n",
       " 'it',\n",
       " '``',\n",
       " 'ama',\n",
       " \"''\",\n",
       " 'in',\n",
       " 'Ayurveda',\n",
       " '–',\n",
       " 'accumulated',\n",
       " 'toxins',\n",
       " 'from',\n",
       " 'undigested',\n",
       " 'foods',\n",
       " '.',\n",
       " 'The',\n",
       " 'person',\n",
       " 'who',\n",
       " 'has',\n",
       " 'it',\n",
       " 'might',\n",
       " 'be',\n",
       " 'ill',\n",
       " '.',\n",
       " 'I',\n",
       " 'scrape',\n",
       " 'my',\n",
       " 'tongue',\n",
       " 'every',\n",
       " 'day',\n",
       " 'From',\n",
       " 'Proto-Indo-Aryan',\n",
       " '*',\n",
       " 'HaHmás',\n",
       " ',',\n",
       " 'from',\n",
       " 'Proto-Indo-Iranian',\n",
       " '*',\n",
       " 'HaHmás',\n",
       " ',',\n",
       " 'from',\n",
       " 'Proto-Indo-European',\n",
       " '*',\n",
       " 'h₂eh₃mós',\n",
       " '(',\n",
       " '“',\n",
       " 'raw',\n",
       " ',',\n",
       " 'uncooked',\n",
       " '”',\n",
       " ')',\n",
       " ',',\n",
       " 'from',\n",
       " '*',\n",
       " 'h₂eh₃-',\n",
       " '(',\n",
       " '“',\n",
       " 'to',\n",
       " 'burn',\n",
       " '”',\n",
       " ')',\n",
       " '.',\n",
       " 'Cognate',\n",
       " 'with',\n",
       " 'Ancient',\n",
       " 'Greek',\n",
       " 'ὠμός',\n",
       " '(',\n",
       " 'ōmós',\n",
       " ',',\n",
       " '“',\n",
       " 'raw',\n",
       " ',',\n",
       " 'crude',\n",
       " ',',\n",
       " 'uncooked',\n",
       " ',',\n",
       " 'undressed',\n",
       " '”',\n",
       " ')',\n",
       " ',',\n",
       " 'Old',\n",
       " 'Armenian',\n",
       " 'հում',\n",
       " '(',\n",
       " 'hum',\n",
       " ',',\n",
       " '“',\n",
       " 'raw',\n",
       " ',',\n",
       " 'uncooked',\n",
       " '”',\n",
       " ')',\n",
       " ',',\n",
       " 'Old',\n",
       " 'Irish',\n",
       " 'om',\n",
       " '(',\n",
       " '“',\n",
       " 'raw',\n",
       " ',',\n",
       " 'uncooked',\n",
       " '”',\n",
       " ')',\n",
       " '(',\n",
       " 'whence',\n",
       " 'Irish',\n",
       " 'amh',\n",
       " ')',\n",
       " ',',\n",
       " 'Persian',\n",
       " 'خام\\u200e',\n",
       " '(',\n",
       " 'xâm',\n",
       " ',',\n",
       " '“',\n",
       " 'crude',\n",
       " ',',\n",
       " 'raw',\n",
       " '”',\n",
       " ')',\n",
       " '.',\n",
       " '(',\n",
       " 'source',\n",
       " ')',\n",
       " 'VHM',\n",
       " ':',\n",
       " 'In',\n",
       " 'some',\n",
       " 'Indic',\n",
       " 'languages',\n",
       " 'it',\n",
       " 'means',\n",
       " ',',\n",
       " 'among',\n",
       " 'other',\n",
       " 'things',\n",
       " ',',\n",
       " '``',\n",
       " 'undigested',\n",
       " \"''\",\n",
       " ',',\n",
       " 'as',\n",
       " 'Heidi',\n",
       " 'noted',\n",
       " 'for',\n",
       " 'Ayurveda',\n",
       " 'in',\n",
       " 'general',\n",
       " '.',\n",
       " 'Diana',\n",
       " 'S.',\n",
       " 'Zhang',\n",
       " '(',\n",
       " 'South',\n",
       " 'Asian',\n",
       " 'Studies',\n",
       " 'graduate',\n",
       " 'student',\n",
       " ')',\n",
       " ':',\n",
       " 'We',\n",
       " 'call',\n",
       " 'it',\n",
       " '舌苔',\n",
       " '(',\n",
       " 'shétái',\n",
       " ',',\n",
       " '“',\n",
       " 'tongue',\n",
       " 'moss',\n",
       " '”',\n",
       " ')',\n",
       " 'in',\n",
       " 'Chinese',\n",
       " '.',\n",
       " 'It',\n",
       " 'does',\n",
       " 'indicate',\n",
       " 'accumulated',\n",
       " 'dietary',\n",
       " 'toxins',\n",
       " ',',\n",
       " 'but',\n",
       " 'in',\n",
       " 'Chinese',\n",
       " 'medicine',\n",
       " 'it',\n",
       " 'even',\n",
       " 'more',\n",
       " 'suggests',\n",
       " '心火',\n",
       " 'xīn',\n",
       " 'huǒ',\n",
       " '(',\n",
       " '“',\n",
       " 'heart',\n",
       " 'fire',\n",
       " \"''\",\n",
       " ')',\n",
       " 'within',\n",
       " 'that',\n",
       " 'person',\n",
       " '—',\n",
       " 'meaning',\n",
       " 'that',\n",
       " 'they',\n",
       " 'must',\n",
       " 'be',\n",
       " 'extremely',\n",
       " 'stressed',\n",
       " ',',\n",
       " 'anxious',\n",
       " ',',\n",
       " 'and',\n",
       " 'in',\n",
       " 'a',\n",
       " 'high-strung',\n",
       " 'mode',\n",
       " 'for',\n",
       " 'an',\n",
       " 'extended',\n",
       " 'period',\n",
       " 'of',\n",
       " 'time',\n",
       " '!',\n",
       " 'Their',\n",
       " '“',\n",
       " 'heart',\n",
       " 'fire',\n",
       " '”',\n",
       " 'must',\n",
       " 'be',\n",
       " 'owing',\n",
       " 'to',\n",
       " 'striving',\n",
       " 'for',\n",
       " 'a',\n",
       " 'position',\n",
       " 'or',\n",
       " 'other',\n",
       " 'much',\n",
       " 'desired',\n",
       " 'goal',\n",
       " '.',\n",
       " 'Poor',\n",
       " 'person',\n",
       " '!',\n",
       " 'But',\n",
       " ',',\n",
       " 'yes',\n",
       " ',',\n",
       " 'they',\n",
       " 'should',\n",
       " 'scrape',\n",
       " 'it',\n",
       " 'off',\n",
       " 'before',\n",
       " 'meeting',\n",
       " 'others',\n",
       " '(',\n",
       " 'I',\n",
       " 'scrape',\n",
       " 'my',\n",
       " 'tongue',\n",
       " 'everyday',\n",
       " '!',\n",
       " ')',\n",
       " '.',\n",
       " 'Hygiene',\n",
       " 'showcases',\n",
       " 'one',\n",
       " \"'s\",\n",
       " 'capacity',\n",
       " 'to',\n",
       " 'take',\n",
       " 'care',\n",
       " 'of',\n",
       " 'oneself',\n",
       " 'and',\n",
       " 'maintain',\n",
       " 'professionalism',\n",
       " 'under',\n",
       " 'all',\n",
       " 'circumstances',\n",
       " '.',\n",
       " 'I',\n",
       " 'wish',\n",
       " 'this',\n",
       " 'person',\n",
       " 'the',\n",
       " 'best',\n",
       " 'of',\n",
       " 'luck',\n",
       " 'in',\n",
       " 'their',\n",
       " 'striving',\n",
       " 'for',\n",
       " 'achievement',\n",
       " '!',\n",
       " 'I',\n",
       " 'think',\n",
       " 'I',\n",
       " 'totally',\n",
       " 'empathize',\n",
       " 'with',\n",
       " 'their',\n",
       " '心火',\n",
       " 'xīn',\n",
       " 'huǒ',\n",
       " '(',\n",
       " '“',\n",
       " 'heart',\n",
       " 'fire',\n",
       " \"''\",\n",
       " ')',\n",
       " 'illness',\n",
       " 'because',\n",
       " 'I',\n",
       " '’',\n",
       " 'm',\n",
       " 'under',\n",
       " 'constant',\n",
       " 'stress',\n",
       " 'from',\n",
       " 'workload',\n",
       " ',',\n",
       " 'too',\n",
       " '.',\n",
       " 'I',\n",
       " 'wonder',\n",
       " 'if',\n",
       " 'there',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'name',\n",
       " 'for',\n",
       " 'this',\n",
       " 'phenomenon',\n",
       " 'in',\n",
       " 'other',\n",
       " 'languages',\n",
       " '.',\n",
       " 'We',\n",
       " \"'re\",\n",
       " 'linguists',\n",
       " ',',\n",
       " 'after',\n",
       " 'all',\n",
       " ',',\n",
       " 'and',\n",
       " 'are',\n",
       " 'preoccupied',\n",
       " 'with',\n",
       " 'matters',\n",
       " 'of',\n",
       " 'the',\n",
       " 'tongue',\n",
       " '.',\n",
       " 'Selected',\n",
       " 'readings',\n",
       " \"''\",\n",
       " 'The',\n",
       " 'pain',\n",
       " 'of',\n",
       " 'forgetting',\n",
       " 'one',\n",
       " \"'s\",\n",
       " 'mother',\n",
       " '/',\n",
       " 'father',\n",
       " 'tongue',\n",
       " \"''\",\n",
       " '(',\n",
       " '9/4/21',\n",
       " ')',\n",
       " 'Mother',\n",
       " 'Tongue',\n",
       " ':',\n",
       " 'lost',\n",
       " 'and',\n",
       " 'found',\n",
       " \"''\",\n",
       " '(',\n",
       " '12/15/14',\n",
       " ')',\n",
       " 'Victor',\n",
       " 'H.',\n",
       " 'Mair',\n",
       " ',',\n",
       " '``',\n",
       " 'How',\n",
       " 'to',\n",
       " 'Forget',\n",
       " 'Your',\n",
       " 'Mother',\n",
       " 'Tongue',\n",
       " 'and',\n",
       " 'Remember',\n",
       " 'Your',\n",
       " 'National',\n",
       " 'Language',\n",
       " \"''\",\n",
       " '(',\n",
       " '2003',\n",
       " ')']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = BeautifulSoup(content).get_text()\n",
    "word_tokenize(raw)\n",
    "\n",
    "# With some further work, we can write programs to create a small corpus of blog posts, and use this as the basis for our NLP work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563eff25",
   "metadata": {},
   "source": [
    "## Reading Local Files\n",
    "\n",
    "In order to read a local file, we need to use Python's built-in open() function, followed by the read() method. Suppose you have a \n",
    "file document.txt, you can load its contents like this:\n",
    "\n",
    "first create a document.txt in c:\\cgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bae0dacf",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'document.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7592/3438269229.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'document.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mraw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'document.txt'"
     ]
    }
   ],
   "source": [
    "f = open('document.txt')\n",
    "raw = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4e40d4",
   "metadata": {},
   "source": [
    "To check that the file that you are trying to open is really in the right directory, use IDLE's Open command in the File menu; this\n",
    "will display a list of all the files in the directory where IDLE is running. An alternative is to examine the current directory \n",
    "from within Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3059971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159668f0",
   "metadata": {},
   "source": [
    "Another possible problem you might have encountered when accessing a text file is the newline conventions, which are different for \n",
    "different operating systems. The built-in open() function has a second parameter for controlling how the file is opened: \n",
    "open('document.txt', 'rU') — 'r' means to open the file for reading (the default), and 'U' stands for \"Universal\", which lets us \n",
    "ignore the different conventions used for marking newlines.\n",
    "\n",
    "Assuming that you can open the file, there are several methods for reading it. The read() method creates a string with the contents\n",
    "of the entire file:\n",
    "\n",
    "- f.read()\n",
    "\n",
    "\n",
    "Recall that the '\\n' characters are newlines; this is equivalent to pressing Enter on a keyboard and starting a new line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391f6a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also read a file one line at a time using a for loop:\n",
    "\n",
    "f = open('document.txt', 'rU')\n",
    "for line in f:\n",
    "    print(line.strip())\n",
    "    \n",
    "# Here we use the strip() method to remove the newline character at the end of the input line.\n",
    "f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23098ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK's corpus files can also be accessed using these methods. We simply have to use nltk.data.find() to get the filename for any \n",
    "# corpus item. Then we can open and read it in the way we just demonstrated above:\n",
    "\n",
    "path = nltk.data.find('corpora/gutenberg/melville-moby_dick.txt')\n",
    "raw = open(path, 'rU').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f818beba",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65314481",
   "metadata": {},
   "source": [
    "We have \"word_tokenize()\", \"sent_tokenize()\" models which we can import from nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89dc1852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'are', 'several', 'fishes', 'in', 'the', 'sea', ',', 'can', 'you', 'see', 'them', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "text = \"There are several fishes in the sea, can you see them ?\"\n",
    "word_token = word_tokenize(text)\n",
    "print(word_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3417505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There are several fishes in the sea, can you see them ?']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "text = \"There are several fishes in the sea, can you see them ?\"\n",
    "sent_token = sent_tokenize(text)\n",
    "print(sent_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa5987b",
   "metadata": {},
   "source": [
    "After illustrating each model how it's done.\n",
    "- Word tokenize: We use the method word_tokenize() to split a sentence into words. The output of word tokenization can be converted to Data Frame for better text understanding in machine learning applications.\n",
    "\n",
    "- Sentence tokenize: We use the sent_tokenize() method to split a document or paragraph into sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ce382e",
   "metadata": {},
   "source": [
    "## White Space Tokenization\n",
    "The simplest way to tokenize text is to use whitespace within a string as the “delimiter” of words. This can be accomplished with Python’s split function, which is available on all string object instances as well as on the string built-in class itself. You can change the separator any way you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7220eb0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'was',\n",
       " 'born',\n",
       " 'in',\n",
       " 'jordan',\n",
       " 'in',\n",
       " '2002,',\n",
       " 'Therefore',\n",
       " 'i',\n",
       " 'am',\n",
       " '19',\n",
       " 'years',\n",
       " 'old']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sentence = \"I was born in jordan in 2002. Therefore i am 19 years old\"\n",
    "Sentence.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fdcbad",
   "metadata": {},
   "source": [
    "As you can notice, this built-in Python method already does a good job tokenizing a simple sentence. It’s “mistake” was on the last word, where it included the sentence-ending punctuation with the token “2002.”. We need the tokens to be separated from neighboring punctuation and other significant tokens in a sentence.\n",
    "\n",
    "In the example below, we’ll perform sentence tokenization using the comma as a separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2f19430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I was born in jordan in 2002', ' Therefore i am 19 years old']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sentence.split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c628574",
   "metadata": {},
   "source": [
    "There are also a couple of tokenizers you can use...\n",
    "for example:-\n",
    "## Punctuation-based tokenizer\n",
    "This tokenizer splits the sentences into words based on whitespaces and punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "692b6f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'was', 'born', 'in', 'jordan', 'in', '2002', ',', 'Therefore', 'i', 'am', '19', 'years', 'old']\n"
     ]
    }
   ],
   "source": [
    "from nltk import wordpunct_tokenize\n",
    "print(wordpunct_tokenize(Sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a81143",
   "metadata": {},
   "source": [
    "## Treebank Word tokenizer\n",
    "This tokenizer incorporates a variety of common rules for english word tokenization. It separates phrase-terminating punctuation like (?!.;,) from adjacent tokens and retains decimal numbers as a single token. Besides, it contains rules for English contractions. \n",
    "\n",
    "For example “don’t” is tokenized as [“do”, “n’t”]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4488c7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'was', 'born', 'in', 'jordan', 'in', '2002', ',', 'Therefore', 'i', 'am', '19', 'years', 'old']\n"
     ]
    }
   ],
   "source": [
    "from nltk import TreebankWordTokenizer\n",
    "Tokenizer = TreebankWordTokenizer()\n",
    "print(Tokenizer.tokenize(Sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86d7ab5",
   "metadata": {},
   "source": [
    "## Extracting Text from PDF, MSWord and other Binary Formats\n",
    "\n",
    "ASCII text and HTML text are human readable formats. Text often comes in binary formats — like PDF and MSWord — that can only be \n",
    "opened using specialized software. Third-party libraries such as pypdf and pywin32 provide access to these formats. Extracting \n",
    "text from multi-column documents is particularly challenging. For once-off conversion of a few documents, it is simpler to open the \n",
    "document with a suitable application, then save it as text to your local drive, and access it as described below. If the document \n",
    "is already on the web, you can enter its URL in Google's search box. The search result often includes a link to an HTML version of \n",
    "the document, which you can save as text.\n",
    "\n",
    "### Capturing User Input\n",
    "\n",
    "Sometimes we want to capture the text that a user inputs when she is interacting with our program. To prompt the user to type a \n",
    "line of input, call the Python function input(). After saving the input to a variable, we can manipulate it just as we have done \n",
    "for other strings.\n",
    "\n",
    "- s = input(\"Enter some text: \")\n",
    "\n",
    "Note: I need to use input() in Python. This was mentioned earlier in NLTK book as well.\n",
    "Note: input works for Python 3. For Python 2, you need to use raw_input...\n",
    "Update NLTK folks with this as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4a7d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = input(\"Enter some text: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd37ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"You typed\", len(word_tokenize(s)), \"words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882e27c7",
   "metadata": {},
   "source": [
    "When we tokenize a string we produce a list (of words), and this is Python's <list> type. Normalizing and sorting lists produces \n",
    "other lists:\n",
    "Note that word_tokenize is from the NLTK library (as imported above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2057bbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(raw)\n",
    "\n",
    "print type(tokens)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73056d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we make all tokens lower case and turn into a new list words\n",
    "\n",
    "words = [w.lower() for w in tokens]\n",
    "print(type(words))\n",
    "print (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc8db22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take our lower case words, apply set (to get the \"set\" of words or vocabulary)\n",
    "# Then we sort it and save to a new variable, vocab\n",
    "\n",
    "vocab = sorted(set(words))\n",
    "print(type(vocab))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a748435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The type of an object determines what operations you can perform on it. So, for example, we can append to a list but not to a \n",
    "# string:\n",
    "\n",
    "vocab.append('blog')\n",
    "print (vocab)\n",
    "\n",
    "# note, every time I rerun this code, I add \"blog\" to the end of it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d65fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.append('blog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b203cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above is an error... HOwever, I could do the following?\n",
    "\n",
    "raw = raw + \" blog\"\n",
    "print (raw)\n",
    "\n",
    "# Yay! I can..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b0f89392",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"list\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7592/3315843671.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# but we cannot concatenate a string to a list...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mquery\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbeatles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"list\") to str"
     ]
    }
   ],
   "source": [
    "# Similarly, we can concatenate strings with strings, and lists with lists, but we cannot concatenate strings with lists:\n",
    "\n",
    "# query is a string\n",
    "query = 'Who knows?'\n",
    "\n",
    "# beatles is a list\n",
    "\n",
    "beatles = ['john', 'paul', 'george', 'ringo']\n",
    "\n",
    "# but we cannot concatenate a string to a list...\n",
    "query + beatles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aa617660",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'beatles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7592/2074051919.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# But, in my estimation, we could append the string to the list as follows:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mbeatles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbeatles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'beatles' is not defined"
     ]
    }
   ],
   "source": [
    "# But, in my estimation, we could append the string to the list as follows:\n",
    "\n",
    "beatles.append(query)\n",
    "print (beatles)\n",
    "\n",
    "# yes, this works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1bcbb6",
   "metadata": {},
   "source": [
    "# 3.3 Text Processing with Unicode\n",
    "\n",
    "Our programs will often need to deal with different languages, and different character sets. The concept\n",
    "of \"plain text\" is a fiction. If you live in the English-speaking world you probably use ASCII, possibly \n",
    "without realizing it. If you live in Europe you might use one of the extended Latin character sets, \n",
    "containing such characters as \"ø\" for Danish and Norwegian, \"ő\" for Hungarian, \"ñ\" for Spanish and \n",
    "Breton, and \"ň\" for Czech and Slovak. In this section, we will give an overview of how to use Unicode \n",
    "for processing texts that use non-ASCII character sets.\n",
    "\n",
    "## What is Unicode?\n",
    "\n",
    "Unicode supports over a million characters. Each character is assigned a number, called a code point. \n",
    "In Python, code points are written in the form \\uXXXX, where XXXX is the number in 4-digit hexadecimal \n",
    "form.\n",
    "\n",
    "Within a program, we can manipulate Unicode strings just like normal strings. However, when Unicode \n",
    "characters are stored in files or displayed on a terminal, they must be encoded as a stream of bytes. \n",
    "Some encodings (such as ASCII and Latin-2) use a single byte per code point, so they can only support \n",
    "a small subset of Unicode, enough for a single language. Other encodings (such as UTF-8) use multiple \n",
    "bytes and can represent the full range of Unicode characters.\n",
    "\n",
    "Text in files will be in a particular encoding, so we need some mechanism for translating it into \n",
    "Unicode — translation into Unicode is called decoding. Conversely, to write out Unicode to a file or a \n",
    "terminal, we first need to translate it into a suitable encoding — this translation out of Unicode is \n",
    "called encoding, and is illustrated in 3.3.\n",
    "\n",
    "From a Unicode perspective, characters are abstract entities which can be realized as one or more \n",
    "glyphs. Only glyphs can appear on a screen or be printed on paper. A font is a mapping from characters \n",
    "to glyphs.\n",
    "\n",
    "### Extracting encoded text from files\n",
    "\n",
    "Let's assume that we have a small text file, and that we know how it is encoded. For example, \n",
    "polish-lat2.txt, as the name suggests, is a snippet of Polish text (from the Polish Wikipedia; see \n",
    "http://pl.wikipedia.org/wiki/Biblioteka_Pruska). This file is encoded as Latin-2, also known as \n",
    "ISO-8859-2. The function nltk.data.find() locates the file for us:\n",
    "\n",
    "-> path = nltk.data.find('corpora/unicode_samples/polish-lat2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5cfdcb85",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7592/4270326487.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'latin2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'path' is not defined"
     ]
    }
   ],
   "source": [
    "# The Python open() function can read encoded data into Unicode strings, and write out Unicode strings \n",
    "# in encoded form. It takes a parameter to specify the encoding of the file being read or written. So \n",
    "# let's open our Polish file with the encoding 'latin2' and inspect the contents of the file:\n",
    "\n",
    "import codecs\n",
    "\n",
    "f = codecs.open(path, encoding='latin2')\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    print(line)\n",
    "    \n",
    "# To open the file with latin-2 encoding in Python 2.X, we need to codecs.open instead of open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "41fceb26",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7592/2861982645.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# two-digit \\xXX and four-digit \\uXXXX representations:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'latin2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'path' is not defined"
     ]
    }
   ],
   "source": [
    "# If this does not display correctly on your terminal, or if we want to see the underlying numerical \n",
    "# values (or \"codepoints\") of the characters, then we can convert all non-ASCII characters into their \n",
    "# two-digit \\xXX and four-digit \\uXXXX representations:\n",
    "\n",
    "f = codecs.open(path, encoding='latin2')\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    print(line.encode('unicode_escape'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d84f39f5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['Two', 'days', 'ago', ',', 'I', 'met', 'a', 'person', 'who', 'had', 'a', 'thick', 'white', 'coating', 'on', 'their', 'tongue', '.', 'Wondering', 'what', 'it', 'was', 'called', 'and', 'its', 'implications', 'for', 'health', ',', 'I', 'asked', 'members', 'of', 'the', 'e-Mair', 'list', 'about', 'it', '.', 'Here', 'are', 'some', 'of', 'the', 'answers', 'I', 'received', ':', 'Denis', '(', 'Sinologist', ')', ':', 'Thick', 'tongue', 'coating', ',', 'often', 'due', 'to', 'lengthening', 'of', 'the', 'keratinous', 'papillae', 'on', 'the', 'tongue', \"'s\", 'surface', '.', 'Heidi', '(', 'Yoga', 'teacher', 'and', 'Ayurveda', 'specialist', ')', ':', 'We', 'call', 'it', '``', 'ama', \"''\", 'in', 'Ayurveda', '–', 'accumulated', 'toxins', 'from', 'undigested', 'foods', '.', 'The', 'person', 'who', 'has', 'it', 'might', 'be', 'ill', '.', 'I', 'scrape', 'my', 'tongue', 'every', 'day', 'From', 'Proto-Indo-Aryan', '*', 'HaHmás', ',', 'from', 'Proto-Indo-Iranian', '*', 'HaHmás', ',', 'from', 'Proto-Indo-European', '*', 'h₂eh₃mós', '(', '“', 'raw', ',', 'uncooked', '”', ')', ',', 'from', '*', 'h₂eh₃-', '(', '“', 'to', 'burn', '”', ')', '.', 'Cognate', 'with', 'Ancient', 'Greek', 'ὠμός', '(', 'ōmós', ',', '“', 'raw', ',', 'crude', ',', 'uncooked', ',', 'undressed', '”', ')', ',', 'Old', 'Armenian', 'հում', '(', 'hum', ',', '“', 'raw', ',', 'uncooked', '”', ')', ',', 'Old', 'Irish', 'om', '(', '“', 'raw', ',', 'uncooked', '”', ')', '(', 'whence', 'Irish', 'amh', ')', ',', 'Persian', 'خام\\u200e', '(', 'xâm', ',', '“', 'crude', ',', 'raw', '”', ')', '.', '(', 'source', ')', 'VHM', ':', 'In', 'some', 'Indic', 'languages', 'it', 'means', ',', 'among', 'other', 'things', ',', '``', 'undigested', \"''\", ',', 'as', 'Heidi', 'noted', 'for', 'Ayurveda', 'in', 'general', '.', 'Diana', 'S.', 'Zhang', '(', 'South', 'Asian', 'Studies', 'graduate', 'student', ')', ':', 'We', 'call', 'it', '舌苔', '(', 'shétái', ',', '“', 'tongue', 'moss', '”', ')', 'in', 'Chinese', '.', 'It', 'does', 'indicate', 'accumulated', 'dietary', 'toxins', ',', 'but', 'in', 'Chinese', 'medicine', 'it', 'even', 'more', 'suggests', '心火', 'xīn', 'huǒ', '(', '“', 'heart', 'fire', \"''\", ')', 'within', 'that', 'person', '—', 'meaning', 'that', 'they', 'must', 'be', 'extremely', 'stressed', ',', 'anxious', ',', 'and', 'in', 'a', 'high-strung', 'mode', 'for', 'an', 'extended', 'period', 'of', 'time', '!', 'Their', '“', 'heart', 'fire', '”', 'must', 'be', 'owing', 'to', 'striving', 'for', 'a', 'position', 'or', 'other', 'much', 'desired', 'goal', '.', 'Poor', 'person', '!', 'But', ',', 'yes', ',', 'they', 'should', 'scrape', 'it', 'off', 'before', 'meeting', 'others', '(', 'I', 'scrape', 'my', 'tongue', 'everyday', '!', ')', '.', 'Hygiene', 'showcases', 'one', \"'s\", 'capacity', 'to', 'take', 'care', 'of', 'oneself', 'and', 'maintain', 'professionalism', 'under', 'all', 'circumstances', '.', 'I', 'wish', 'this', 'person', 'the', 'best', 'of', 'luck', 'in', 'their', 'striving', 'for', 'achievement', '!', 'I', 'think', 'I', 'totally', 'empathize', 'with', 'their', '心火', 'xīn', 'huǒ', '(', '“', 'heart', 'fire', \"''\", ')', 'illness', 'because', 'I', '’', 'm', 'under', 'constant', 'stress', 'from', 'workload', ',', 'too', '.', 'I', 'wonder', 'if', 'there', \"'s\", 'a', 'name', 'for', 'this', 'phenomenon', 'in', 'other', 'languages', '.', 'We', \"'re\", 'linguists', ',', 'after', 'all', ',', 'and', 'are', 'preoccupied', 'with', 'matters', 'of', 'the', 'tongue', '.', 'Selected', 'readings', \"''\", 'The', 'pain', 'of', 'forgetting', 'one', \"'s\", 'mother', '/', 'father', 'tongue', \"''\", '(', '9/4/21', ')', 'Mother', 'Tongue', ':', 'lost', 'and', 'found', \"''\", '(', '12/15/14', ')', 'Victor', 'H.', 'Mair', ',', '``', 'How', 'to', 'Forget', 'Your', 'Mother', 'Tongue', 'and', 'Remember', 'Your', 'National', 'Language', \"''\", '(', '2003', ')']\n"
     ]
    }
   ],
   "source": [
    "# When we tokenize a string we produce a list (of words), and this is Python's <list> type. Normalizing and sorting lists produces \n",
    "# other lists:\n",
    "\n",
    "# Note that word_tokenize is from the NLTK library (as imported above)\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(raw)\n",
    "\n",
    "print(type(tokens))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "be086a06",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['two', 'days', 'ago', ',', 'i', 'met', 'a', 'person', 'who', 'had', 'a', 'thick', 'white', 'coating', 'on', 'their', 'tongue', '.', 'wondering', 'what', 'it', 'was', 'called', 'and', 'its', 'implications', 'for', 'health', ',', 'i', 'asked', 'members', 'of', 'the', 'e-mair', 'list', 'about', 'it', '.', 'here', 'are', 'some', 'of', 'the', 'answers', 'i', 'received', ':', 'denis', '(', 'sinologist', ')', ':', 'thick', 'tongue', 'coating', ',', 'often', 'due', 'to', 'lengthening', 'of', 'the', 'keratinous', 'papillae', 'on', 'the', 'tongue', \"'s\", 'surface', '.', 'heidi', '(', 'yoga', 'teacher', 'and', 'ayurveda', 'specialist', ')', ':', 'we', 'call', 'it', '``', 'ama', \"''\", 'in', 'ayurveda', '–', 'accumulated', 'toxins', 'from', 'undigested', 'foods', '.', 'the', 'person', 'who', 'has', 'it', 'might', 'be', 'ill', '.', 'i', 'scrape', 'my', 'tongue', 'every', 'day', 'from', 'proto-indo-aryan', '*', 'hahmás', ',', 'from', 'proto-indo-iranian', '*', 'hahmás', ',', 'from', 'proto-indo-european', '*', 'h₂eh₃mós', '(', '“', 'raw', ',', 'uncooked', '”', ')', ',', 'from', '*', 'h₂eh₃-', '(', '“', 'to', 'burn', '”', ')', '.', 'cognate', 'with', 'ancient', 'greek', 'ὠμός', '(', 'ōmós', ',', '“', 'raw', ',', 'crude', ',', 'uncooked', ',', 'undressed', '”', ')', ',', 'old', 'armenian', 'հում', '(', 'hum', ',', '“', 'raw', ',', 'uncooked', '”', ')', ',', 'old', 'irish', 'om', '(', '“', 'raw', ',', 'uncooked', '”', ')', '(', 'whence', 'irish', 'amh', ')', ',', 'persian', 'خام\\u200e', '(', 'xâm', ',', '“', 'crude', ',', 'raw', '”', ')', '.', '(', 'source', ')', 'vhm', ':', 'in', 'some', 'indic', 'languages', 'it', 'means', ',', 'among', 'other', 'things', ',', '``', 'undigested', \"''\", ',', 'as', 'heidi', 'noted', 'for', 'ayurveda', 'in', 'general', '.', 'diana', 's.', 'zhang', '(', 'south', 'asian', 'studies', 'graduate', 'student', ')', ':', 'we', 'call', 'it', '舌苔', '(', 'shétái', ',', '“', 'tongue', 'moss', '”', ')', 'in', 'chinese', '.', 'it', 'does', 'indicate', 'accumulated', 'dietary', 'toxins', ',', 'but', 'in', 'chinese', 'medicine', 'it', 'even', 'more', 'suggests', '心火', 'xīn', 'huǒ', '(', '“', 'heart', 'fire', \"''\", ')', 'within', 'that', 'person', '—', 'meaning', 'that', 'they', 'must', 'be', 'extremely', 'stressed', ',', 'anxious', ',', 'and', 'in', 'a', 'high-strung', 'mode', 'for', 'an', 'extended', 'period', 'of', 'time', '!', 'their', '“', 'heart', 'fire', '”', 'must', 'be', 'owing', 'to', 'striving', 'for', 'a', 'position', 'or', 'other', 'much', 'desired', 'goal', '.', 'poor', 'person', '!', 'but', ',', 'yes', ',', 'they', 'should', 'scrape', 'it', 'off', 'before', 'meeting', 'others', '(', 'i', 'scrape', 'my', 'tongue', 'everyday', '!', ')', '.', 'hygiene', 'showcases', 'one', \"'s\", 'capacity', 'to', 'take', 'care', 'of', 'oneself', 'and', 'maintain', 'professionalism', 'under', 'all', 'circumstances', '.', 'i', 'wish', 'this', 'person', 'the', 'best', 'of', 'luck', 'in', 'their', 'striving', 'for', 'achievement', '!', 'i', 'think', 'i', 'totally', 'empathize', 'with', 'their', '心火', 'xīn', 'huǒ', '(', '“', 'heart', 'fire', \"''\", ')', 'illness', 'because', 'i', '’', 'm', 'under', 'constant', 'stress', 'from', 'workload', ',', 'too', '.', 'i', 'wonder', 'if', 'there', \"'s\", 'a', 'name', 'for', 'this', 'phenomenon', 'in', 'other', 'languages', '.', 'we', \"'re\", 'linguists', ',', 'after', 'all', ',', 'and', 'are', 'preoccupied', 'with', 'matters', 'of', 'the', 'tongue', '.', 'selected', 'readings', \"''\", 'the', 'pain', 'of', 'forgetting', 'one', \"'s\", 'mother', '/', 'father', 'tongue', \"''\", '(', '9/4/21', ')', 'mother', 'tongue', ':', 'lost', 'and', 'found', \"''\", '(', '12/15/14', ')', 'victor', 'h.', 'mair', ',', '``', 'how', 'to', 'forget', 'your', 'mother', 'tongue', 'and', 'remember', 'your', 'national', 'language', \"''\", '(', '2003', ')']\n"
     ]
    }
   ],
   "source": [
    "# Here we make all tokens lower case and turn into a new list words\n",
    "\n",
    "words = [w.lower() for w in tokens]\n",
    "print(type(words))\n",
    "print (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42fb248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take our lower case words, apply set (to get the \"set\" of words or vocabulary)\n",
    "# Then we sort it and save to a new variable, vocab\n",
    "\n",
    "vocab = sorted(set(words))\n",
    "print(type(vocab))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcca6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The type of an object determines what operations you can perform on it. So, for example, we can append to a list but not to a \n",
    "# string:\n",
    "\n",
    "vocab.append('blog')\n",
    "print (vocab)\n",
    "\n",
    "# note, every time I rerun this code, I add \"blog\" to the end of it..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b56fca1",
   "metadata": {},
   "source": [
    "## Using your local encoding in Python\n",
    "If you are used to working with characters in a particular local encoding, you probably want to be able to use your standard methods for inputting and editing strings in a Python file. In order to do this, you need to include the string '# -*- coding: <coding> -*-' as the first or second line of your file. Note that <coding> has to be a string like 'latin-1', 'big5' or 'utf-8'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2951a7",
   "metadata": {},
   "source": [
    "## Finding Word Stems using Regular expression\n",
    "When we use a web search engine, we usually don't mind (or even notice) if the words in the document differ from our search terms in having different endings. A query for laptops finds documents containing laptop and vice versa. Indeed, laptop and laptops are just two forms of the same dictionary word (or lemma). For some language processing tasks we want to ignore word endings, and just deal with word stems.\n",
    "\n",
    "There are various ways we can pull out the stem of a word. Here's a simple-minded approach which just strips off anything that looks like a suffix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a30db72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "     for suffix in ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']:\n",
    "         if word.endswith(suffix):\n",
    "             return word[:-len(suffix)]\n",
    "     return word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae64f5d",
   "metadata": {},
   "source": [
    "Although we will ultimately use NLTK's built-in stemmers, it's interesting to see how we can use regular expressions for this task. Our first step is to build up a disjunction of all the suffixes. We need to enclose it in parentheses in order to limit the scope of the disjunction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed3b4cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ing']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.findall(r'^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a60b5b3",
   "metadata": {},
   "source": [
    "Here, re.findall() just gave us the suffix even though the regular expression matched the entire word. This is because the parentheses have a second function, to select substrings to be extracted. If we want to use the parentheses to specify the scope of the disjunction, but not to select the material to be output, we have to add \"?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "892a81dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['processing']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^.*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb5194a",
   "metadata": {},
   "source": [
    "However, we'd actually like to split the word into stem and suffix. So we should just parenthesize both parts of the regular expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fc4f163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('process', 'ing')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a457dd",
   "metadata": {},
   "source": [
    "This looks promising, but still has a problem. Let's look at a different word, processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94dc8c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('processe', 's')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d1390a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DENNIS',\n",
       " ':',\n",
       " 'Listen',\n",
       " ',',\n",
       " 'strange',\n",
       " 'women',\n",
       " 'ly',\n",
       " 'in',\n",
       " 'pond',\n",
       " 'distribut',\n",
       " 'swordsi',\n",
       " 'no',\n",
       " 'basi',\n",
       " 'for',\n",
       " 'a',\n",
       " 'system',\n",
       " 'of',\n",
       " 'govern',\n",
       " '.',\n",
       " 'Supreme',\n",
       " 'execut',\n",
       " 'power',\n",
       " 'deriv',\n",
       " 'froma',\n",
       " 'mandate',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mass',\n",
       " ',',\n",
       " 'not',\n",
       " 'from',\n",
       " 'some',\n",
       " 'farcical',\n",
       " 'aquatic',\n",
       " 'ceremony',\n",
       " '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This approach still has many problems (can you spot them?) but we will move on to define a function to perform stemming, and apply it to a whole text:\n",
    "\n",
    " \t\n",
    "def stem(word):\n",
    "     regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
    "     stem, suffix = re.findall(regexp, word)[0]\n",
    "     return stem\n",
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swordsis no basis for a system of government.  Supreme executive power derives froma mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "tokens = word_tokenize(raw)\n",
    "[stem(t) for t in tokens]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b0ae35",
   "metadata": {},
   "source": [
    "Notice that our regular expression removed the s from ponds but also from is and basis. It produced some non-words like distribut and deriv, but these are acceptable stems in some applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee43be0",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- We view a text as a list of words. A \"raw text\" is a potentially long string containing words and whitespace formatting, and is how we typically store and visualize a text.\n",
    "- A string is specified in Python using single or double quotes: 'Monty Python', \"Monty Python\".\n",
    "The characters of a string are accessed using indexes, counting from zero: 'Monty Python'[0] gives the value M. The length of a string is found using len().\n",
    "- Substrings are accessed using slice notation: 'Monty Python'[1:5] gives the value onty. If the start index is omitted, the substring begins at the start of the string; if the end index is omitted, the slice continues to the end of the string.\n",
    "- Strings can be split into lists: 'Monty Python'.split() gives ['Monty', 'Python']. Lists can be joined into strings: '/'.join(['Monty', 'Python']) gives 'Monty/Python'.\n",
    "- We can read text from a file input.txt using text = open('input.txt').read(). We can read text from url using text = request.urlopen(url).read().decode('utf8'). We can iterate over the lines of a text file using for line in open(f).\n",
    "- We can write text to a file by opening the file for writing output_file = open('output.txt', 'w'), then adding content to the file print(\"Monty Python\", file=output_file).\n",
    "- Texts found on the web may contain unwanted material (such as headers, footers, markup), that need to be removed before we do any linguistic processing.\n",
    "- Tokenization is the segmentation of a text into basic units — or tokens — such as words and punctuation. Tokenization based on whitespace is inadequate for many applications because it bundles punctuation together with words. NLTK provides an off-the-shelf tokenizer nltk.word_tokenize().\n",
    "- Lemmatization is a process that maps the various forms of a word (such as appeared, appears) to the canonical or citation form of the word, also known as the lexeme or lemma (e.g. appear).\n",
    "- Regular expressions are a powerful and flexible method of specifying patterns. Once we have imported the re module, we can use re.findall() to find all substrings in a string that match a pattern.\n",
    "- If a regular expression string includes a backslash, you should tell Python not to preprocess the string, by using a raw string with an r prefix: r'regexp'.\n",
    "- When backslash is used before certain characters, e.g. \\n, this takes on a special meaning (newline character); however, when backslash is used before regular expression wildcards and operators, e.g. \\., \\|, \\$, these characters lose their special meaning and are matched literally.\n",
    "- A string formatting expression template % arg_tuple consists of a format string template that contains conversion specifiers like %-6s and %0.2d."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81468f9d",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "☼ Define a string s = 'colorless'. Write a Python statement that changes this to \"colourless\" using only the slice and concatenation operations.\n",
    "\n",
    "☼ We can use the slice notation to remove morphological endings on words. For example, 'dogs'[:-1] removes the last character of dogs, leaving dog. Use slice notation to remove the affixes from these words (we've inserted a hyphen to indicate the affix boundary, but omit this from your strings): dish-es, run-ning, nation-ality, un-do, pre-heat.\n",
    "\n",
    "☼ We saw how we can generate an IndexError by indexing beyond the end of a string. Is it possible to construct an index that goes too far to the left, before the start of the string?\n",
    "\n",
    "☼ We can specify a \"step\" size for the slice. The following returns every second character within the slice: monty[6:11:2]. It also works in the reverse direction: monty[10:5:-2] Try these for yourself, then experiment with different step values.\n",
    "\n",
    "☼ What happens if you ask the interpreter to evaluate monty[::-1]? Explain why this is a reasonable result.\n",
    "\n",
    "☼ Describe the class of strings matched by the following regular expressions.\n",
    "\n",
    "- [a-zA-Z]+\n",
    "- [A-Z][a-z]*\n",
    "- p[aeiou]{,2}t\n",
    "- \\d+(\\.\\d+)?\n",
    "- ([^aeiou][aeiou][^aeiou])*\n",
    "- \\w+|[^\\w\\s]+\n",
    "Test your answers using nltk.re_show().\n",
    "\n",
    "☼ Write regular expressions to match the following classes of strings:\n",
    "\n",
    "A single determiner (assume that a, an, and the are the only determiners).\n",
    "An arithmetic expression using integers, addition, and multiplication, such as 2*3+8.\n",
    "☼ Write a utility function that takes a URL as its argument, and returns the contents of the URL, with all HTML markup removed. Use from urllib import request and then request.urlopen('http://nltk.org/').read().decode('utf8') to access the contents of the URL.\n",
    "\n",
    "☼ Save some text into a file corpus.txt. Define a function load(f) that reads from the file named in its sole argument, and returns a string containing the text of the file.\n",
    "\n",
    "Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the various kinds of punctuation in this text. Use one multi-line regular expression, with inline comments, using the verbose flag (?x).\n",
    "Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the following kinds of expression: monetary amounts; dates; names of people and organizations.\n",
    "☼ Rewrite the following loop as a list comprehension:\n",
    "The code is 1.1 under the exercises\n",
    "\n",
    "☼ Write a for loop to print out the characters of a string, one per line.\n",
    "\n",
    "☼ What is the difference between calling split on a string with no argument or with ' ' as the argument, e.g. sent.split() versus sent.split(' ')? What happens when the string being split contains tab characters, consecutive space characters, or a sequence of tabs and spaces? (In IDLE you will need to use '\\t' to enter a tab character.)\n",
    "\n",
    "☼ Create a variable words containing a list of words. Experiment with words.sort() and sorted(words). What is the difference?\n",
    "\n",
    "☼ Explore the difference between strings and integers by typing the following at a Python prompt: \"3\" * 7 and 3 * 7. Try converting between strings and integers using int(\"3\") and str(3).\n",
    "\n",
    "☼ Use a text editor to create a file called prog.py containing the single line monty = 'Monty Python'. Next, start up a new session with the Python interpreter, and enter the expression monty at the prompt. You will get an error from the interpreter. Now, try the following (note that you have to leave off the .py part of the filename):\n",
    "The code is 1.2 under the exercises\n",
    "\n",
    "☼ What happens when the formatting strings %6s and %-6s are used to display strings that are longer than six characters?\n",
    "\n",
    "◑ Read in some text from a corpus, tokenize it, and print the list of all wh-word types that occur. (wh-words in English are used in questions, relative clauses and exclamations: who, which, what, and so on.) Print them in order. Are any words duplicated in this list, because of the presence of case distinctions or punctuation?\n",
    "\n",
    "◑ Create a file consisting of words and (made up) frequencies, where each line consists of a word, the space character, and a positive integer, e.g. fuzzy 53. Read the file into a Python list using open(filename).readlines(). Next, break each line into its two fields using split(), and convert the number into an integer using int(). The result should be a list of the form: [['fuzzy', 53], ...].\n",
    "\n",
    "◑ Write code to access a favorite webpage and extract some text from it. For example, access a weather site and extract the forecast top temperature for your town or city today.\n",
    "\n",
    "◑ Write a function unknown() that takes a URL as its argument, and returns a list of unknown words that occur on that webpage. In order to do this, extract all substrings consisting of lowercase letters (using re.findall()) and remove any items from this set that occur in the Words Corpus (nltk.corpus.words). Try to categorize these words manually and discuss your findings.\n",
    "\n",
    "◑ Examine the results of processing the URL http://news.bbc.co.uk/ using the regular expressions suggested above. You will see that there is still a fair amount of non-textual data there, particularly Javascript commands. You may also find that sentence breaks have not been properly preserved. Define further regular expressions that improve the extraction of text from this web page.\n",
    "\n",
    "◑ Are you able to write a regular expression to tokenize text in such a way that the word don't is tokenized into do and n't? Explain why this regular expression won't work: «n't|\\w+».\n",
    "\n",
    "◑ Try to write code to convert text into hAck3r, using regular expressions and substitution, where e → 3, i → 1, o → 0, l → |, s → 5, . → 5w33t!, ate → 8. Normalize the text to lowercase before converting it. Add more substitutions of your own. Now try to map s to two different values: $ for word-initial s, and 5 for word-internal s.\n",
    "\n",
    "◑ Pig Latin is a simple transformation of English text. Each word of the text is converted as follows: move any consonant (or consonant cluster) that appears at the start of the word to the end, then append ay, e.g. string → ingstray, idle → idleay. http://en.wikipedia.org/wiki/Pig_Latin\n",
    "\n",
    "Write a function to convert a word to Pig Latin.\n",
    "Write code that converts text, instead of individual words.\n",
    "Extend it further to preserve capitalization, to keep qu together (i.e. so that quiet becomes ietquay), and to detect when y is used as a consonant (e.g. yellow) vs a vowel (e.g. style).\n",
    "◑ Download some text from a language that has vowel harmony (e.g. Hungarian), extract the vowel sequences of words, and create a vowel bigram table.\n",
    "\n",
    "◑ Python's random module includes a function choice() which randomly chooses an item from a sequence, e.g. choice(\"aehh \") will produce one of four possible characters, with the letter h being twice as frequent as the others. Write a generator expression that produces a sequence of 500 randomly chosen letters drawn from the string \"aehh \", and put this expression inside a call to the ''.join() function, to concatenate them into one long string. You should get a result that looks like uncontrolled sneezing or maniacal laughter: he  haha ee  heheeh eha. Use split() and join() again to normalize the whitespace in this string.\n",
    "\n",
    "◑ Consider the numeric expressions in the following sentence from the MedLine Corpus: The corresponding free cortisol fractions in these sera were 4.53 +/- 0.15% and 8.16 +/- 0.23%, respectively. Should we say that the numeric expression 4.53 +/- 0.15% is three words? Or should we say that it's a single compound word? Or should we say that it is actually nine words, since it's read \"four point five three, plus or minus zero point fifteen percent\"? Or should we say that it's not a \"real\" word at all, since it wouldn't appear in any dictionary? Discuss these different possibilities. Can you think of application domains that motivate at least two of these answers?\n",
    "\n",
    "◑ Readability measures are used to score the reading difficulty of a text, for the purposes of selecting texts of appropriate difficulty for language learners. Let us define μw to be the average number of letters per word, and μs to be the average number of words per sentence, in a given text. The Automated Readability Index (ARI) of the text is defined to be: 4.71 μw + 0.5 μs - 21.43. Compute the ARI score for various sections of the Brown Corpus, including section f (lore) and j (learned). Make use of the fact that nltk.corpus.brown.words() produces a sequence of words, while nltk.corpus.brown.sents() produces a sequence of sentences.\n",
    "\n",
    "◑ Use the Porter Stemmer to normalize some tokenized text, calling the stemmer on each word. Do the same thing with the Lancaster Stemmer and see if you observe any differences.\n",
    "\n",
    "◑ Define the variable saying to contain the list ['After', 'all', 'is', 'said', 'and', 'done', ',', 'more',\n",
    "'is', 'said', 'than', 'done', '.']. Process this list using a for loop, and store the length of each word in a new list lengths. Hint: begin by assigning the empty list to lengths, using lengths = []. Then each time through the loop, use append() to add another length value to the list. Now do the same thing using a list comprehension.\n",
    "\n",
    "◑ Define a variable silly to contain the string: 'newly formed bland ideas are inexpressible in an infuriating\n",
    "way'. (This happens to be the legitimate interpretation that bilingual English-Spanish speakers can assign to Chomsky's famous nonsense phrase, colorless green ideas sleep furiously according to Wikipedia). Now write code to perform the following tasks:\n",
    "\n",
    "Split silly into a list of strings, one per word, using Python's split() operation, and save this to a variable called bland.\n",
    "Extract the second letter of each word in silly and join them into a string, to get 'eoldrnnnna'.\n",
    "Combine the words in bland back into a single string, using join(). Make sure the words in the resulting string are separated with whitespace.\n",
    "Print the words of silly in alphabetical order, one per line.\n",
    "◑ The index() function can be used to look up items in sequences. For example, 'inexpressible'.index('e') tells us the index of the first position of the letter e.\n",
    "\n",
    "What happens when you look up a substring, e.g. 'inexpressible'.index('re')?\n",
    "Define a variable words containing a list of words. Now use words.index() to look up the position of an individual word.\n",
    "Define a variable silly as in the exercise above. Use the index() function in combination with list slicing to build a list phrase consisting of all the words up to (but not including) in in silly.\n",
    "◑ Write code to convert nationality adjectives like Canadian and Australian to their corresponding nouns Canada and Australia (see http://en.wikipedia.org/wiki/List_of_adjectival_forms_of_place_names).\n",
    "\n",
    "◑ Read the LanguageLog post on phrases of the form as best as p can and as best p can, where p is a pronoun. Investigate this phenomenon with the help of a corpus and the findall() method for searching tokenized text described in 3.5. http://itre.cis.upenn.edu/~myl/languagelog/archives/002733.html\n",
    "\n",
    "◑ Study the lolcat version of the book of Genesis, accessible as nltk.corpus.genesis.words('lolcat.txt'), and the rules for converting text into lolspeak at http://www.lolcatbible.com/index.php?title=How_to_speak_lolcat. Define regular expressions to convert English words into corresponding lolspeak words.\n",
    "\n",
    "◑ Read about the re.sub() function for string substitution using regular expressions, using help(re.sub) and by consulting the further readings for this chapter. Use re.sub in writing code to remove HTML tags from an HTML file, and to normalize whitespace.\n",
    "\n",
    "★ An interesting challenge for tokenization is words that have been split across a line-break. E.g. if long-term is split, then we have the string long-\\nterm.\n",
    "\n",
    "Write a regular expression that identifies words that are hyphenated at a line-break. The expression will need to include the \\n character.\n",
    "Use re.sub() to remove the \\n character from these words.\n",
    "How might you identify words that should not remain hyphenated once the newline is removed, e.g. 'encyclo-\\npedia'?x\n",
    "★ Read the Wikipedia entry on Soundex. Implement this algorithm in Python.\n",
    "\n",
    "★ Obtain raw texts from two or more genres and compute their respective reading difficulty scores as in the earlier exercise on reading difficulty. E.g. compare ABC Rural News and ABC Science News (nltk.corpus.abc). Use Punkt to perform sentence segmentation.\n",
    "\n",
    "★ Rewrite the following nested loop as a nested list comprehension:\n",
    "The code 1.3 is under the exercises\n",
    " \t\n",
    "★ Use WordNet to create a semantic index for a text collection. Extend the concordance search program in 3.6, indexing each word using the offset of its first synset, e.g. wn.synsets('dog')[0].offset (and optionally the offset of some of its ancestors in the hypernym hierarchy).\n",
    "\n",
    "★ With the help of a multilingual corpus such as the Universal Declaration of Human Rights Corpus (nltk.corpus.udhr), and NLTK's frequency distribution and rank correlation functionality (nltk.FreqDist, nltk.spearman_correlation), develop a system that guesses the language of a previously unseen text. For simplicity, work with a single character encoding and just a few languages.\n",
    "\n",
    "★ Write a program that processes a text and discovers cases where a word has been used with a novel sense. For each word, compute the WordNet similarity between all synsets of the word and all synsets of the words in its context. (Note that this is a crude approach; doing it well is a difficult, open research problem.)\n",
    "\n",
    "★ Read the article on normalization of non-standard words (Sproat et al, 2001), and implement a similar system for text normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c29fd30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 3),\n",
       " ('dog', 3),\n",
       " ('gave', 4),\n",
       " ('John', 4),\n",
       " ('the', 3),\n",
       " ('newspaper', 9)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code 1.1\n",
    ">>> sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    ">>> result = []\n",
    ">>> for word in sent:\n",
    "...     word_len = (word, len(word))\n",
    "...     result.append(word_len)\n",
    ">>> result\n",
    "[('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]\n",
    "# Define a string raw containing a sentence of your own choosing. Now, split raw on some character other than space, such as 's'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9aa3c0b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'prog'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5720/2345101176.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Code 1.2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mprog\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmonty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmonty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# This time, Python should return with a value. You can also try import prog, in which case Python should be able to evaluate the expression prog.monty at the prompt.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'prog'"
     ]
    }
   ],
   "source": [
    "# Code 1.2\n",
    ">>> from prog import monty\n",
    ">>> monty\n",
    "# This time, Python should return with a value. You can also try import prog, in which case Python should be able to evaluate the expression prog.monty at the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8a460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code 1.3\n",
    ">>> words = ['attribution', 'confabulation', 'elocution',\n",
    "...          'sequoia', 'tenacious', 'unidirectional']\n",
    ">>> vsequences = set()\n",
    ">>> for word in words:\n",
    "...     vowels = []\n",
    "...     for char in word:\n",
    "...         if char in 'aeiou':\n",
    "...             vowels.append(char)\n",
    "...     vsequences.add(''.join(vowels))\n",
    ">>> sorted(vsequences)\n",
    "['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
