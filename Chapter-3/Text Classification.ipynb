{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0, 100, 200, .1); padding: 1em 3em; border-radius: 5px; border: 1px solid black\">\n",
    "    <div style=\"font-weight: bold; font-size: 1.2em; border-bottom: 1px dashed black; padding-bottom: .5em;\">\n",
    "        Machine Learning for NLP\n",
    "    </div>\n",
    "    <p>We learned about various extraction methods, such as tokenization, stemming, lemmatization, and stop-word removal, which are used to extract features from unstructured text. We also discussed Bag of Words and Term Frequency-Inverse Document Frequency (TFIDF).\n",
    "\n",
    "In this session, we will learn how to use these extracted features to develop machine learning models. These models are capable of solving real-world problems, such as detecting whether sentiments carried by texts are positive or negative, predicting whether emails are spam or not, and so on. We will also cover concepts such as supervised and unsupervised learning, classifications and regressions, sampling and splitting data, along with evaluating the performance of a model in depth. This chapter also discusses how to load and save these models for future use.</p>\n",
    "    <ol>\n",
    "        <li>Supervised Learning</li>\n",
    "         <li>Unsupervised Learning</li>\n",
    "         <li>Semi-supervised Learning</li>\n",
    "        <li>Re-inforcement</li>\n",
    "    </ol>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0, 100, 200, .1); padding: 1em 3em; border-radius: 5px; border: 1px solid black\">\n",
    "    <div style=\"font-weight: bold; font-size: 1.2em; border-bottom: 1px dashed black; padding-bottom: .5em;\">\n",
    "        Classification\n",
    "    </div>\n",
    "    <p>Say you have two types of food, of which type 1 tastes sweet and type 2 tastes salty, and you need to determine how an unknown food will taste using various attributes of the food (such as color, fragrance, shape, and ingredients). This is an instance of classification.\n",
    "\n",
    "Here, the two classes are class 1, which tastes sweet, and class 2, which tastes salty. The features that are used in this classification are color, fragrance, the ingredients used to prepare the dish, and so on. These features are called independent variables. The class (sweet or salty) is called a dependent variable.\n",
    "\n",
    "Formally, classification algorithms are those that learn patterns from a given dataset to determine classes of unknown datasets. Some of the most widely used classification algorithms are logistic regression, Naive Bayes, k-nearest neighbor, and tree methods. Let's learn about each of them</p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert a new cell and add the following code to import the necessary packages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0, 100, 200, .1); padding: 1em 3em; border-radius: 5px; border: 1px solid black\">\n",
    "    <div style=\"font-weight: bold; font-size: 1.2em; border-bottom: 1px dashed black; padding-bottom: .5em;\">\n",
    "        Logistic Regression\n",
    "    </div>\n",
    "    <p>Despite having the term \"regression\" in it, logistic regression is used for probabilistic classification. In this case, the dependent variable (the outcome) is binary, which means that the values can be represented by 0 or 1. For example, consider that you need to decide whether an email is spam or not. Here, the value of the decision (the dependent variable, or the outcome) can be considered to be 1 if the email is spam; otherwise, it will be 0. No other outcome is possible. The independent variables (that is, the features) will consist of various attributes of the email, such as the number of occurrences of certain keywords and so on. We can then make use of the logistic regression algorithm to create a model that predicts if the email is spam (1) or not (0), as shown in the following graph:</p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from pylab import *\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the data file in JSON format using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data = pd.read_json('data/reviews_Musical_Instruments_5.json', lines=True)\n",
    "review_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data[['reviewText', 'overall']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use a lambda function to extract tokens from each 'reviewText' of this DataFrame\n",
    "We will then lemmatize them, and concatenate them side by side Use the join function to concatenate a list of words into a single sentence. Use the regular expression method (re) to replace anything other than alphabetical characters, digits, and whitespaces with blank space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "review_data['cleaned_review_text'] = review_data['reviewText'].apply(\\\n",
    "lambda x : ' '.join([lemmatizer.lemmatize(word.lower()) \\\n",
    "    for word in word_tokenize(re.sub(r'([^\\s\\w]|_)+', ' ', str(x)))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import download\n",
    "download('stopwords')\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "len(review_data['cleaned_review_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data['cleaned_text'] = review_data['reviewText'].apply(\\\n",
    "lambda x : ' '.join([lemmatizer.lemmatize(word.lower()) \\\n",
    "    for word in word_tokenize(re.sub(r'([^\\s\\w]|_)+', ' ', str(x))) if word.lower() not in stop_words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a DataFrame from the TFIDF matrix representation of the cleaned version of reviewText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data[['cleaned_review_text', 'reviewText', 'overall']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a TFIDF matrix and transform it into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = TfidfVectorizer(max_features=500)\n",
    "tfidf_df = pd.DataFrame(tfidf_model.fit_transform(review_data['cleaned_review_text']).todense())\n",
    "tfidf_df.columns = sorted(tfidf_model.vocabulary_)\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data['target'] = review_data['overall'].apply(lambda x : 0 if x<=4 else 1)\n",
    "review_data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Ideintify X and Y\n",
    "X = tfidf_df\n",
    "y = review_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training_features, test_features, \\\n",
    "training_target, test_target, = train_test_split(X,y,\n",
    "                                               test_size = .2,\n",
    "                                               random_state= 45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use sklearn's LogisticRegression() function to fit a logistic regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Initiate the model\n",
    "logreg = LogisticRegression()\n",
    "# Train the Model on 80%\n",
    "logreg.fit(training_features, training_target)\n",
    "#  Test the Model on 20%\n",
    "lr_predicted_labels = logreg.predict(test_features)\n",
    "# Transform the probability into binary ouput\n",
    "logreg.predict_proba(test_features)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crosstab function to create a cross validation table\n",
    "Compater the actual target variable with predicted target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review_data['predicted_labels'] = predicted_labels\n",
    "# pd.crosstab(review_data['target'], review_data['predicted_labels'])\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "CMLR= confusion_matrix(test_target, lr_predicted_labels)\n",
    "CMLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Score\n",
    "LR = accuracy_score(test_target, lr_predicted_labels)\n",
    "\n",
    "print(\" Logistic Regression Prediction Accuracy : {:.2f}%\".format(LR * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0, 100, 200, .1); padding: 1em 3em; border-radius: 5px; border: 1px solid black\">\n",
    "    <div style=\"font-weight: bold; font-size: 1.2em; border-bottom: 1px dashed black; padding-bottom: .5em;\">\n",
    "        Naive Bayes Classifiers\n",
    "    </div>\n",
    "    <p>ust like logistic regression, a Naive Bayes classifier is another kind of probabilistic classifier. It is based on Bayes' theorem.\n",
    "\n",
    "In the preceding formula, A and B are events and P(B) is not equal to 0. P(A/B) is the probability of event A occurring, given that event B is true. Similarly, P(B/A) is the probability of event B occurring, given that event A is true. P(B) is the probability of the occurrence of event B.\n",
    "\n",
    "Say there is an online platform where hotel customers can provide a review for the service provided to them. The hotel now wants to figure out whether new reviews on the platform are appreciative in nature or not. Here, P(A) = the probability of the review being an appreciative one, while P(B) = the probability of the review being long. Now, we've come across a review that is long and want to figure out the probability of it being appreciative. To do that, we need to calculate P(A/B). P(B/A) will be the probability of appreciative reviews being long. From the training dataset, we can easily calculate P(B/A), P(A), and P(B) and then use Bayes' theorem to calculate P(A/B).\n",
    "\n",
    "Similar to logistic regression, the scikit-learn library can be used to perform naïve Bayes classification and can be implemented in Python using the following code:</p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert a new cell and add the following code to import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# Initiate the model\n",
    "nb = GaussianNB()\n",
    "# Train the Model on 80%\n",
    "nb.fit(training_features, training_target)\n",
    "#  Test the Model on 20%\n",
    "nb_predicted_labels = nb.predict(test_features)\n",
    "# Transform the probability into binary ouput\n",
    "nb.predict_proba(test_features)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "CMNB= confusion_matrix(test_target, nb_predicted_labels)\n",
    "CMNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Score\n",
    "NB = accuracy_score(test_target, nb_predicted_labels)\n",
    "\n",
    "print(\" Naive Bayes Prediction Accuracy : {:.2f}%\".format(NB * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0, 100, 200, .1); padding: 1em 3em; border-radius: 5px; border: 1px solid black\">\n",
    "    <div style=\"font-weight: bold; font-size: 1.2em; border-bottom: 1px dashed black; padding-bottom: .5em;\">\n",
    "        k-nearest Neighbors\n",
    "    </div>\n",
    "    <p>k-nearest neighbors is an algorithm that can be used to solve both regression and classification. In this chapter, we will focus on the classification aspect of the algorithm as it is used for NLP applications. Consider, for instance, the saying \"birds of a feather flock together.\" This means that people who have similar interests prefer to stay close to each other and form groups. This characteristic is called homophily. This characteristic is the main idea behind the k-nearest neighbors classification algorithm.\n",
    "\n",
    "To classify an unknown object, k number of other objects located nearest to it with class labels will be looked into. The class that occurs the most among them will be assigned to it—that is, the object with an unknown class. The value of k is chosen by running experiments on the training dataset to find the most optimal value. When dealing with text data for a given document, we interpret \"nearest neighbors\" as other documents that are the most similar to the unknown document.</p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert a new cell and add the following code to import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Initiate the model\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "# Train the Model on 80%\n",
    "knn.fit(training_features, training_target)\n",
    "#  Test the Model on 20%\n",
    "knn_predicted_labels = knn.predict(test_features)\n",
    "# Transform the probability into binary ouput\n",
    "knn.predict_proba(test_features)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "CMKNN = confusion_matrix(test_target, knn_predicted_labels)\n",
    "CMKNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Score\n",
    "KNN = accuracy_score(test_target, knn_predicted_labels)\n",
    "\n",
    "print(\" KNN Prediction Accuracy : {:.2f}%\".format(NB * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models compared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" Logistic Regression Prediction Accuracy : {:.2f}%\".format(LR * 100))\n",
    "print(\" Naive Bayes Prediction Accuracy         : {:.2f}%\".format(NB * 100))\n",
    "print(\" KNN Prediction Accuracy                 : {:.2f}%\".format(NB * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0, 100, 200, .1); padding: 1em 3em; border-radius: 5px; border: 1px solid black\">\n",
    "    <div style=\"font-weight: bold; font-size: 1.2em; border-bottom: 1px dashed black; padding-bottom: .5em;\">\n",
    "        Regression\n",
    "    </div>\n",
    "    <p>To better understand regression, consider a practical example. For example, say you have photos of several people, along with a list of their respective ages, and you need to predict the ages of some other people from their photos. This is a use case for regression.\n",
    "\n",
    "In the case of regression, the dependent variable (age, in this example) is continuous. The independent variables—that is, features—consist of the attributes of the images, such as the color intensity of each pixel. Formally, regression analysis refers to the process of learning a mapping function, which relates features or predictors (inputs) to the dependent variable (output).\n",
    "\n",
    "There are various types of regression: univariate, multivariate, simple, multiple, linear, non-linear, polynomial regression, stepwise regression, ridge regression, lasso regression, and elastic net regression. If there is just one dependent variable, then it is referred to as univariate regression. On the other hand, two or more dependent variables constitute multivariate regression. Simple regression has only one predictor or target variable, while multivariate regression has more than one predictor variable.\n",
    "\n",
    "Since linear regression in the base algorithm for all the different types of regression mentioned previously, in the next section, we will cover linear regression in detail.</p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0, 100, 200, .1); padding: 1em 3em; border-radius: 5px; border: 1px solid black\">\n",
    "    <div style=\"font-weight: bold; font-size: 1.2em; border-bottom: 1px dashed black; padding-bottom: .5em;\">\n",
    "        Linear Regression\n",
    "    </div>\n",
    "    <p>The term \"linear\" refers to the linearity of parameters. Parameters are the coefficients of predictor variables in the linear regression equation. The following formula represents the linear regression equation:\n",
    "\n",
    "Here, y is termed a dependent variable (output); it is continuous. X is an independent variable or feature (input). β0 and β1 are parameters. Є is the error component, which is the difference between the actual and predicted values of y. Since linear regression requires the variable to be linear, it is not used much in the real world. However, it is useful for high-level predictions, such as the sales revenue of a product given the price and advertising cost.</p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the original data again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = TfidfVectorizer(max_features=500)\n",
    "tfidf_df = pd.DataFrame(tfidf_model.fit_transform(review_data['cleaned_review_text']).todense())\n",
    "tfidf_df.columns = sorted(tfidf_model.vocabulary_)\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Ideintify X and Y\n",
    "X = tfidf_df\n",
    "y = review_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training_features, test_features, \\\n",
    "training_target, test_target, = train_test_split(X,y,\n",
    "                                               test_size = .2,\n",
    "                                               random_state= 45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use sklearn's LinearRegression() function to fit a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(training_features,training_target)\n",
    "linreg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To check the intercept or the error term of the linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To check the prediction in a TFIDF DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the model to predict the 'overall' score and store it in a column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features['predicted_score_from_linear_regression'] = linreg.predict(test_features)\n",
    "test_features[['overall', 'predicted_score_from_linear_regression']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will analyze the text documents in sklearn's fetch_20newsgroups dataset. The 20 newsgroups dataset contains news articles on 20 different topics. We will make use of hierarchical clustering to classify the documents into different groups. Once the clusters have been created, we will compare them with their actual categories. Follow these steps to implement this exercise:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0, 100, 200, .1); padding: 1em 3em; border-radius: 5px; border: 1px solid black\">\n",
    "    <div style=\"font-weight: bold; font-size: 1.2em; border-bottom: 1px dashed black; padding-bottom: .5em;\">\n",
    "        Performing Hierarchical Clustering\n",
    "    </div>\n",
    "    <p>Hierarchical clustering algorithms group similar objects together to create a cluster with the help of a dendrogram. In this algorithm, we can vary the number of clusters as per our requirements. First, we construct a matrix consisting of distances between each pair of instances (data points). After that, we construct a dendrogram (a representation of clusters in the form of a tree) based on the distances between them. We truncate the tree at a location corresponding to the number of clusters we need.\n",
    "\n",
    "For example, imagine that you have 10 documents and want to group them into a number of categories based on their attributes (the number of words they contain, the number of paragraphs, punctuation, and so on) and don't have any fixed number of categories in mind. This is a use case of hierarchical clustering. Let's assume that we have a dataset containing the features of the 10 documents. Firstly, the distances between each pair of documents from the set of 10 documents are calculated. After that, we construct a dendrogram and truncate it at a suitable position to get a suitable number of clusters:</p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert a new cell and add the following code to import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "import matplotlib as mpl\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from pylab import *\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download a list of stop words and the Wordnet corpus from nltk. Insert a new cell and add the following code to implement this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words=stopwords.words('english')\n",
    "stop_words=stop_words+list(string.printable)\n",
    "nltk.download('wordnet')\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify the categories of news articles we want to fetch to perform our clustering task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories= ['misc.forsale', 'sci.electronics', 'talk.religion.misc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To fetch the dataset, add the following lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42, download_if_missing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To view the data of the fetched content, add the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data['data'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To check the categories of news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news_data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To store news_data and the corresponding categories in a pandas DataFrame and view it, write the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data_df = pd.DataFrame({'text' : news_data['data'], 'category': news_data.target})\n",
    "news_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To count the number of occurrences of each category appearing in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data_df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use a lambda function to extract tokens from each \"text\" of the news_data_df DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data_df['cleaned_text'] = news_data_df['text'].apply(\\\n",
    "lambda x : ' '.join([lemmatizer.lemmatize(word.lower()) \\\n",
    "    for word in word_tokenize(re.sub(r'([^\\s\\w]|_)+', ' ', str(x))) if word.lower() not in stop_words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a TFIDF matrix and transform it into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = TfidfVectorizer(max_features=200)\n",
    "tfidf_df = pd.DataFrame(tfidf_model.fit_transform(news_data_df['cleaned_text']).todense())\n",
    "tfidf_df.columns = sorted(tfidf_model.vocabulary_)\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the distance using the sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances as euclidean\n",
    "dist = 1 - euclidean(tfidf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, create a dendrogram for the TFIDF representation of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dendrogram = sch.dendrogram(sch.linkage(dist, method='ward'))\n",
    "\n",
    "plt.xlabel('Data Points')\n",
    "plt.ylabel('Euclidean Distance')\n",
    "plt.title('Dendrogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above image, we can analyze the high-level patterns that the clustering algorithm found to group the articles into one of the four clusters. As you can see, cluster 2 has mostly religion-related articles, while cluster 3 consists of primarily sales-related articles. The other two clusters do not have a proper distinction. The reason for this could be that the model figured out that words related to \"religion\" and \"for sale\" appeared frequently in the articles that were classified into those respective clusters, while the articles on \"electronics\" consist of mostly generic words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the fcluster() function to obtain the cluster labels of the clusters that were obtained by hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "clusters = fcluster(sch.linkage(dist, method='ward'), k, criterion='maxclust')\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data_df['obtained_clusters'] = clusters\n",
    "pd.crosstab(news_data_df['category'].replace(\n",
    "    {0:'misc.forsale', \n",
    "     1:'sci.electronics', \n",
    "     2:'talk.religion.misc'}),\\\n",
    "            news_data_df['obtained_clusters'].replace(\n",
    "    {1 : 'cluster_1', \n",
    "     2 : 'cluster_2', \n",
    "     3 : 'cluster_3', \n",
    "     4: 'cluster_4'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0, 100, 200, .1); padding: 1em 3em; border-radius: 5px; border: 1px solid black\">\n",
    "    <div style=\"font-weight: bold; font-size: 1.2em; border-bottom: 1px dashed black; padding-bottom: .5em;\">\n",
    "        k-means Clustering\n",
    "    </div>\n",
    "    <p>In this algorithm, we segregate the given instances (data points) into \"k\" number of groups (here, k is a natural number). First, we choose k centroids. We assign each instance to its nearest centroid, thereby creating k groups. This is the assignment phase, which is followed by the update phase.\n",
    "\n",
    "In the update phase, new centroids for each of these k groups are calculated. The data points are reassigned to their nearest newly calculated centroids. The assignment phase and the update phase are carried on repeatedly until the assignment of data points no longer changes.\n",
    "\n",
    "For example, suppose you have 10 documents. You want to group them into three categories based on their attributes, such as the number of words they contain, the number of paragraphs, punctuation, and the tone of the document. In this case, we will assume that k is 3; that is, we want to create these three groups. Firstly, three centroids need to be chosen. In the initialization phase, each of these 10 documents is assigned to one of these three categories, thereby forming three groups. In the update phase, the centroids of the three newly formed groups are calculated. To decide the optimal number of clusters (that is, k), we execute k-means clustering for various values of k and note down their performances (sum of squared errors). We try to select a small value for k that has the lowest sum of squared errors. This method is called the elbow method.</p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will create four clusters from text documents in sklearn's fetch_20newsgroups text dataset using k-means clustering. We will compare these clusters with the actual categories and use the elbow method to obtain the optimal number of clusters. Follow these steps to implement this exercise:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use pandas' crosstab function to compare the clusters we have obtained with the actual categories of the news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data_df['obtained_clusters'] = clusters\n",
    "pd.crosstab(news_data_df['category'].replace(\n",
    "    {0:'misc.forsale', \n",
    "     1:'sci.electronics', \n",
    "     2:'talk.religion.misc'}),\\\n",
    "            news_data_df['obtained_clusters'].replace(\n",
    "    {1 : 'cluster_1', \n",
    "     2 : 'cluster_2', \n",
    "     3 : 'cluster_3', \n",
    "     4: 'cluster_4'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain the optimal value of k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "K = range(1,6)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k)\n",
    "    kmeanModel.fit(tfidf_df)\n",
    "    distortions.append(sum(np.min(cdist(tfidf_df, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / tfidf_df.shape[0])\n",
    "\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal number of clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from pylab import *\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_patio_lawn_garden = pd.read_json('data/reviews_Patio_Lawn_and_Garden_5.json', lines = True)\n",
    "data_patio_lawn_garden[['reviewText', 'overall']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "data_patio_lawn_garden['cleaned_review_text'] = data_patio_lawn_garden['reviewText'].apply(\\\n",
    "lambda x : ' '.join([lemmatizer.lemmatize(word.lower()) \\\n",
    "    for word in word_tokenize(re.sub(r'([^\\s\\w]|_)+', ' ', str(x)))]))\n",
    "data_patio_lawn_garden[['cleaned_review_text', 'reviewText', 'overall']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = TfidfVectorizer(max_features=500)\n",
    "tfidf_df = pd.DataFrame(tfidf_model.fit_transform(data_patio_lawn_garden['cleaned_review_text']).todense())\n",
    "tfidf_df.columns = sorted(tfidf_model.vocabulary_)\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_patio_lawn_garden['target'] = data_patio_lawn_garden['overall'].apply(lambda x : 0 if x<=4 else 1)\n",
    "data_patio_lawn_garden['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "dtc = tree.DecisionTreeClassifier()\n",
    "dtc = dtc.fit(tfidf_df, data_patio_lawn_garden['target'])\n",
    "data_patio_lawn_garden['predicted_labels_dtc'] = dtc.predict(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(data_patio_lawn_garden['target'], data_patio_lawn_garden['predicted_labels_dtc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "dtr = tree.DecisionTreeRegressor()\n",
    "dtr = dtr.fit(tfidf_df, data_patio_lawn_garden['overall'])\n",
    "data_patio_lawn_garden['predicted_values_dtr'] = dtr.predict(tfidf_df)\n",
    "data_patio_lawn_garden[['predicted_values_dtr', 'overall']].head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
